1
00:00:01,780 --> 00:00:05,280
We now start with our agenda
of developing continuous

2
00:00:05,280 --> 00:00:07,970
counterparts of everything
we have done for

3
00:00:07,970 --> 00:00:10,160
discrete random variables.

4
00:00:10,160 --> 00:00:12,710
Let us look at the concept
of expectation.

5
00:00:12,710 --> 00:00:16,030
In the discrete case, we have
defined expectation as a

6
00:00:16,030 --> 00:00:20,400
weighted average of the values
X of the random variable,

7
00:00:20,400 --> 00:00:24,540
weighted according to their
corresponding probabilities.

8
00:00:24,540 --> 00:00:27,450
In the continuous case, we
define expectation in a

9
00:00:27,450 --> 00:00:28,890
similar way--

10
00:00:28,890 --> 00:00:33,820
as a weighted average over the
possible values of X, weighted

11
00:00:33,820 --> 00:00:38,020
according to the corresponding
value of the density.

12
00:00:38,020 --> 00:00:40,370
Points where the density
is higher--

13
00:00:40,370 --> 00:00:42,430
for example, here--

14
00:00:42,430 --> 00:00:46,730
will receive a higher weight
in this calculation.

15
00:00:46,730 --> 00:00:50,620
But of course, since we are
averaging over a continuous

16
00:00:50,620 --> 00:00:55,770
set, the summation will have to
be replaced by an integral.

17
00:00:55,770 --> 00:00:58,190
This will be a recurrent
theme in this unit.

18
00:00:58,190 --> 00:01:01,500
Definitions or formulas for
the continuous case look

19
00:01:01,500 --> 00:01:05,580
exactly like the discrete ones,
except that PMFs are

20
00:01:05,580 --> 00:01:08,730
replaced by densities,
as here.

21
00:01:08,730 --> 00:01:11,170
The PMF is replaced
by a density.

22
00:01:11,170 --> 00:01:15,600
And summations are replaced
by integrals.

23
00:01:15,600 --> 00:01:18,380
The intuition is usually the
same in both the discrete and

24
00:01:18,380 --> 00:01:19,680
the continuous case.

25
00:01:19,680 --> 00:01:23,030
However, the intuition is
usually much clearer, much

26
00:01:23,030 --> 00:01:26,330
easier to visualize in
the discrete case.

27
00:01:26,330 --> 00:01:30,180
So the best strategy is to make
sure to understand fully

28
00:01:30,180 --> 00:01:35,140
the intuition for the discrete
case and just rely on it.

29
00:01:35,140 --> 00:01:39,350
At this point, let me add
some fine print--

30
00:01:39,350 --> 00:01:41,630
a mathematical side point.

31
00:01:41,630 --> 00:01:45,780
This integral or the expectation
will not be always

32
00:01:45,780 --> 00:01:46,600
well defined.

33
00:01:46,600 --> 00:01:50,350
For this integral to make sense,
we will need to make

34
00:01:50,350 --> 00:01:53,620
the assumption that the integral
of the absolute value

35
00:01:53,620 --> 00:01:57,140
of little x, weighted according
to the density,

36
00:01:57,140 --> 00:01:59,500
gives us a finite result.

37
00:01:59,500 --> 00:02:02,540
Unless we explicitly say
something different, we will

38
00:02:02,540 --> 00:02:05,220
always assume that we're dealing
with random variables

39
00:02:05,220 --> 00:02:06,730
that satisfy this condition.

40
00:02:06,730 --> 00:02:11,130
And so the expectation is well
defined mathematically.

41
00:02:11,130 --> 00:02:13,520
Coming back to the big
picture, regarding

42
00:02:13,520 --> 00:02:16,600
expectations, the intuition
remains the same as in the

43
00:02:16,600 --> 00:02:19,620
discrete case-- that the
expectation represents the

44
00:02:19,620 --> 00:02:23,300
average of the values we expect
to see in a very large

45
00:02:23,300 --> 00:02:26,770
number of independent
repetitions of the experiment.

46
00:02:26,770 --> 00:02:30,030
In fact, there are also theorems
to this effect, but

47
00:02:30,030 --> 00:02:33,400
these will have to wait until
later in this class when we

48
00:02:33,400 --> 00:02:35,670
study limit theorems.

49
00:02:35,670 --> 00:02:38,200
Another intuitive interpretation
that is true

50
00:02:38,200 --> 00:02:41,510
for both the discrete and the
continuous case is that the

51
00:02:41,510 --> 00:02:45,100
expectation corresponds to the
center of gravity of the

52
00:02:45,100 --> 00:02:46,829
probability distribution.

53
00:02:46,829 --> 00:02:50,450
So in this diagram, it might
be somewhere around here.

54
00:02:50,450 --> 00:02:53,130
And similarly, for the
continuous diagram, the center

55
00:02:53,130 --> 00:02:57,680
of gravity might be somewhere
around here.

56
00:02:57,680 --> 00:03:01,160
And if it happens that the
distribution, the PMF or the

57
00:03:01,160 --> 00:03:05,580
PDF, happens to be symmetric
around a certain point, then

58
00:03:05,580 --> 00:03:08,930
that point will be equal
to the expectation.

59
00:03:08,930 --> 00:03:12,020
Expectations of continuous
random variables have all the

60
00:03:12,020 --> 00:03:14,030
properties you might expect.

61
00:03:14,030 --> 00:03:17,640
For example, non-negative
random variables have

62
00:03:17,640 --> 00:03:19,970
non-negative expectations.

63
00:03:19,970 --> 00:03:23,600
Random variables that lie
inside an interval have

64
00:03:23,600 --> 00:03:27,030
average values or expectations
that also lie

65
00:03:27,030 --> 00:03:28,920
inside the same interval.

66
00:03:28,920 --> 00:03:33,270
The derivation is exactly the
same as for the discrete case.

67
00:03:33,270 --> 00:03:35,329
There is also an expected
value rule.

68
00:03:35,329 --> 00:03:38,620
In the discrete case, it
took on this form.

69
00:03:38,620 --> 00:03:42,390
In the continuous case, we
obtain an analogous form in

70
00:03:42,390 --> 00:03:45,790
which the summation is replaced
by [an] integral.

71
00:03:45,790 --> 00:03:49,960
And instead of weighing
according to the PMF, we now

72
00:03:49,960 --> 00:03:53,240
weigh according to the
density function.

73
00:03:53,240 --> 00:03:56,090
The derivation of the expected
value rule for the continuous

74
00:03:56,090 --> 00:03:59,155
case is a little more
complicated than the one that

75
00:03:59,155 --> 00:04:01,230
we gave for the discrete case.

76
00:04:01,230 --> 00:04:06,620
But it's sufficient for us to
know that it is true and that

77
00:04:06,620 --> 00:04:10,380
it has an intuitive meaning that
runs along the same lines

78
00:04:10,380 --> 00:04:14,910
as the intuitive meaning that we
had for the discrete case.

79
00:04:14,910 --> 00:04:18,930
As an instance of how we might
apply the expected value rule,

80
00:04:18,930 --> 00:04:22,890
if you wish to calculate the
expected value of the square

81
00:04:22,890 --> 00:04:25,250
of a continuous random
variable, you

82
00:04:25,250 --> 00:04:26,820
would proceed as follows.

83
00:04:26,820 --> 00:04:32,330
You would integrate over the
entire real line the value of

84
00:04:32,330 --> 00:04:37,000
the function, which is X squared
in our case, weighted

85
00:04:37,000 --> 00:04:38,250
according to the density.

86
00:04:41,290 --> 00:04:43,190
Finally, a most important
property of

87
00:04:43,190 --> 00:04:45,870
expectations, is linearity.

88
00:04:45,870 --> 00:04:48,540
Linearity is still true
for continuous random

89
00:04:48,540 --> 00:04:50,100
variables as well.

90
00:04:50,100 --> 00:04:53,200
And the way it is derived is
exactly the same as in the

91
00:04:53,200 --> 00:04:54,470
discrete case.

92
00:04:54,470 --> 00:04:58,510
Namely, we apply the expected
value rule to this function of

93
00:04:58,510 --> 00:05:01,100
the random variable
X and separate

94
00:05:01,100 --> 00:05:04,650
out the various terms.

95
00:05:04,650 --> 00:05:08,200
The story regarding variances is
exactly the same as in the

96
00:05:08,200 --> 00:05:09,630
discrete case.

97
00:05:09,630 --> 00:05:13,010
We define variances using
the same definition.

98
00:05:13,010 --> 00:05:16,210
And of course, here, mu stands
for the expected value of the

99
00:05:16,210 --> 00:05:18,570
random variable X.

100
00:05:18,570 --> 00:05:22,470
To calculate the variance, we
can use the expected value

101
00:05:22,470 --> 00:05:25,860
rule, which takes this form
in the continuous case.

102
00:05:25,860 --> 00:05:29,230
And we apply the expected value
rule for the case where

103
00:05:29,230 --> 00:05:31,570
we're dealing with the expected
value of this

104
00:05:31,570 --> 00:05:34,630
particular function, so that
in this instance, the

105
00:05:34,630 --> 00:05:39,240
functions g of x is x
minus mu squared.

106
00:05:39,240 --> 00:05:43,409
So by applying the expected
value rule, we obtain the

107
00:05:43,409 --> 00:05:47,360
integral from minus infinity to
infinity, the functions g

108
00:05:47,360 --> 00:05:54,550
of x, weighted according to the
density, and then we carry

109
00:05:54,550 --> 00:05:57,960
out the integration.

110
00:05:57,960 --> 00:06:00,180
We also define the standard
deviation--

111
00:06:00,180 --> 00:06:03,480
same way as in the
discrete case.

112
00:06:03,480 --> 00:06:07,360
We have a property about a
variance of linear functions,

113
00:06:07,360 --> 00:06:11,600
of a random variable, namely,
that if we add a constant to a

114
00:06:11,600 --> 00:06:14,740
random variable, this has no
effect on the variance.

115
00:06:14,740 --> 00:06:17,850
But if we multiply a random
variable by a constant, the

116
00:06:17,850 --> 00:06:22,110
variance gets multiplied by the
square of that constant.

117
00:06:22,110 --> 00:06:25,030
Finally, when calculating the
variance, it is often

118
00:06:25,030 --> 00:06:29,810
convenient to use this
alternative formula in which

119
00:06:29,810 --> 00:06:34,360
the variance is calculated by
finding the expected value of

120
00:06:34,360 --> 00:06:37,740
the square of the random
variable and also using the

121
00:06:37,740 --> 00:06:40,980
expected value of the random
variable, but squared and

122
00:06:40,980 --> 00:06:44,880
subtracted from the
first term.

123
00:06:44,880 --> 00:06:48,650
This relation and this relation
are both derived

124
00:06:48,650 --> 00:06:51,920
exactly the same way as
in the discrete case.

125
00:06:51,920 --> 00:06:54,560
And there's no reason to repeat
those derivations.