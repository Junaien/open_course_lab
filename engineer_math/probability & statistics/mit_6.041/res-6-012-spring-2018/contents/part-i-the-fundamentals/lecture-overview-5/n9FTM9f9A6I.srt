1
00:00:00,000 --> 00:00:03,610
In the previous lecture we
introduced random variables,

2
00:00:03,610 --> 00:00:07,050
probability mass functions
and expectations.

3
00:00:07,050 --> 00:00:10,170
In this lecture we continue
with the development of

4
00:00:10,170 --> 00:00:14,190
various concepts associated
with random variables.

5
00:00:14,190 --> 00:00:16,950
There will be three
main parts.

6
00:00:16,950 --> 00:00:19,780
In the first part we define
the variance of a random

7
00:00:19,780 --> 00:00:22,750
variable, and calculate
it for some of our

8
00:00:22,750 --> 00:00:25,150
familiar random variables.

9
00:00:25,150 --> 00:00:28,050
Basically the variance is a
quantity that measures the

10
00:00:28,050 --> 00:00:31,900
amount of spread, or the
dispersion of a probability

11
00:00:31,900 --> 00:00:33,310
mass functions.

12
00:00:33,310 --> 00:00:36,780
In some sense, it quantifies the
amount of randomness that

13
00:00:36,780 --> 00:00:38,210
is present.

14
00:00:38,210 --> 00:00:41,870
Together with the expected
value, the variance summarizes

15
00:00:41,870 --> 00:00:44,890
crisply some of the qualitative
properties of the

16
00:00:44,890 --> 00:00:47,840
probability mass function.

17
00:00:47,840 --> 00:00:51,500
In the second part we discuss
conditioning.

18
00:00:51,500 --> 00:00:54,670
Every probabilistic concept or
result has a conditional

19
00:00:54,670 --> 00:00:55,850
counterpart.

20
00:00:55,850 --> 00:00:58,680
And this is true for probability
mass functions,

21
00:00:58,680 --> 00:01:01,250
expectations and variances.

22
00:01:01,250 --> 00:01:04,160
We define these conditional
counterparts and then develop

23
00:01:04,160 --> 00:01:06,850
the total expectation theorem.

24
00:01:06,850 --> 00:01:11,270
This is a powerful tool that
extends our familiar total

25
00:01:11,270 --> 00:01:15,050
probability theorem and allows
us to divide and conquer when

26
00:01:15,050 --> 00:01:17,820
we calculate expectations.

27
00:01:17,820 --> 00:01:21,360
We then take the opportunity
to dive deeper into the

28
00:01:21,360 --> 00:01:25,450
properties of geometric random
variables, and use a trick

29
00:01:25,450 --> 00:01:27,990
based on the total expectation
theorem to

30
00:01:27,990 --> 00:01:31,039
calculate their mean.

31
00:01:31,039 --> 00:01:35,300
In the last part we show how to
describe probabilistically

32
00:01:35,300 --> 00:01:39,520
the relation between multiple
random variables.

33
00:01:39,520 --> 00:01:42,570
This is done through a so-called
joint probability

34
00:01:42,570 --> 00:01:44,360
mass function.

35
00:01:44,360 --> 00:01:47,380
We take the occasion to
generalize the expected value

36
00:01:47,380 --> 00:01:51,280
rule, and establish a further
linearity property of

37
00:01:51,280 --> 00:01:52,990
expectations.

38
00:01:52,990 --> 00:01:56,650
We finally illustrate the power
of these tools through

39
00:01:56,650 --> 00:02:00,060
the calculation of the expected
value of a binomial

40
00:02:00,060 --> 00:02:01,310
random variable.