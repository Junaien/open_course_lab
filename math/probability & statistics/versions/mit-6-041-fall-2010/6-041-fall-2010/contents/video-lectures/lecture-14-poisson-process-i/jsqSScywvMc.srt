1
00:00:00,040 --> 00:00:02,460
The following content is
provided under a Creative

2
00:00:02,460 --> 00:00:03,870
Commons license.

3
00:00:03,870 --> 00:00:06,910
Your support will help MIT
OpenCourseWare continue to

4
00:00:06,910 --> 00:00:10,560
offer high quality educational
resources for free.

5
00:00:10,560 --> 00:00:13,460
To make a donation or view
additional materials from

6
00:00:13,460 --> 00:00:17,390
hundreds of MIT courses, visit
MIT OpenCourseWare at

7
00:00:17,390 --> 00:00:18,640
ocw.mit.edu.

8
00:00:22,790 --> 00:00:24,630
PROFESSOR: So last time
we started talking

9
00:00:24,630 --> 00:00:26,420
about random processes.

10
00:00:26,420 --> 00:00:30,710
A random process is a random
experiment that

11
00:00:30,710 --> 00:00:32,570
evolves over time.

12
00:00:32,570 --> 00:00:35,110
And conceptually, it's important
to realize that it's

13
00:00:35,110 --> 00:00:37,760
a single probabilistic
experiment

14
00:00:37,760 --> 00:00:39,250
that has many stages.

15
00:00:39,250 --> 00:00:42,440
Actually, it has an infinite
number of stages.

16
00:00:42,440 --> 00:00:47,060
And we discussed the simplest
random process there is, the

17
00:00:47,060 --> 00:00:50,270
Bernoulli process, which is
nothing but the sequence of

18
00:00:50,270 --> 00:00:51,570
Bernoulli trials--

19
00:00:51,570 --> 00:00:54,580
an infinite sequence of
Bernoulli trials.

20
00:00:54,580 --> 00:00:58,250
For example, flipping a
coin over and over.

21
00:00:58,250 --> 00:01:01,640
Once we understand what's going
on with that process,

22
00:01:01,640 --> 00:01:05,519
then what we want is to move
into a continuous time version

23
00:01:05,519 --> 00:01:06,750
of the Bernoulli process.

24
00:01:06,750 --> 00:01:09,420
And this is what we will call
the Poisson process.

25
00:01:09,420 --> 00:01:11,970
And for the Poisson process,
we're going to do exactly the

26
00:01:11,970 --> 00:01:14,300
same things that we did for
the Bernoulli process.

27
00:01:14,300 --> 00:01:18,210
That is, talk about the number
of arrivals during a given

28
00:01:18,210 --> 00:01:21,400
time period, and talk also
about the time between

29
00:01:21,400 --> 00:01:24,160
consecutive arrivals, and
for the distribution of

30
00:01:24,160 --> 00:01:27,660
inter-arrival times.

31
00:01:27,660 --> 00:01:30,660
So let's start with a quick
review of what we

32
00:01:30,660 --> 00:01:32,680
discussed last time.

33
00:01:32,680 --> 00:01:35,500
First, a note about language.

34
00:01:35,500 --> 00:01:38,100
If you think of coin tosses,
we then talk

35
00:01:38,100 --> 00:01:40,660
about heads and tails.

36
00:01:40,660 --> 00:01:43,430
If you think of these as a
sequence of trials, you can

37
00:01:43,430 --> 00:01:47,145
talk about successes
and failures.

38
00:01:47,145 --> 00:01:50,312
The language that we will be
using will be more the

39
00:01:50,312 --> 00:01:51,800
language of arrivals.

40
00:01:51,800 --> 00:01:56,020
That is, if in a given slot you
have a success, you say

41
00:01:56,020 --> 00:01:57,930
that something arrived.

42
00:01:57,930 --> 00:02:00,470
If you have a failure,
nothing arrived.

43
00:02:00,470 --> 00:02:03,090
And that language is a little
more convenient and more

44
00:02:03,090 --> 00:02:06,820
natural, especially when we talk
about continuous time--

45
00:02:06,820 --> 00:02:10,250
to talk about arrivals
instead of successes.

46
00:02:10,250 --> 00:02:12,640
But in any case, for the
Bernoulli process let's keep,

47
00:02:12,640 --> 00:02:14,990
for a little bit, the language
of successes.

48
00:02:14,990 --> 00:02:18,530
Whereas working in discrete
time, we have time slots.

49
00:02:18,530 --> 00:02:20,870
During each time slot,
we have an

50
00:02:20,870 --> 00:02:22,810
independent Bernoulli trial.

51
00:02:22,810 --> 00:02:25,750
There is probability p
of having a success.

52
00:02:25,750 --> 00:02:29,400
Different slots are independent
of each other.

53
00:02:29,400 --> 00:02:33,190
And this probability p is the
same for any given time slot.

54
00:02:33,190 --> 00:02:36,540
So for this process we will
discuss the one random

55
00:02:36,540 --> 00:02:39,240
variable of interest, which
is the following.

56
00:02:39,240 --> 00:02:43,060
If we have n time slots,
or n trials, how many

57
00:02:43,060 --> 00:02:44,600
arrivals will there be?

58
00:02:44,600 --> 00:02:46,990
Or how many successes
will there be?

59
00:02:46,990 --> 00:02:50,910
Well, this is just given
by the binomial PMF.

60
00:02:50,910 --> 00:02:54,780
Number of successes in n trials
is a random variable

61
00:02:54,780 --> 00:02:58,420
that has a binomial PMF, and
we know what this is.

62
00:02:58,420 --> 00:03:01,150
Then we talked about
inter-arrival times.

63
00:03:01,150 --> 00:03:04,490
The time until the first
arrival happens has a

64
00:03:04,490 --> 00:03:07,890
geometric distribution.

65
00:03:07,890 --> 00:03:11,770
And we have seen that
from some time ago.

66
00:03:11,770 --> 00:03:14,890
Now if you start thinking
about the time until k

67
00:03:14,890 --> 00:03:20,040
arrivals happen, and we denote
that by Yk, this is the time

68
00:03:20,040 --> 00:03:22,190
until the first arrival
happens.

69
00:03:22,190 --> 00:03:24,860
And then after the first arrival
happens, you have to

70
00:03:24,860 --> 00:03:27,000
wait some time until
the second arrival

71
00:03:27,000 --> 00:03:28,730
happens, and so on.

72
00:03:28,730 --> 00:03:32,650
And then the time from the
(k -1)th arrival, until

73
00:03:32,650 --> 00:03:34,820
arrival number k.

74
00:03:34,820 --> 00:03:37,680
The important thing to realize
here is that because the

75
00:03:37,680 --> 00:03:41,830
process has a memorylessness
property, once the first

76
00:03:41,830 --> 00:03:45,330
arrival comes, it's as if we're
starting from scratch

77
00:03:45,330 --> 00:03:48,030
and we will be flipping
our coins until the

78
00:03:48,030 --> 00:03:49,560
next arrival comes.

79
00:03:49,560 --> 00:03:52,350
So the time it will take until
the next arrival comes will

80
00:03:52,350 --> 00:03:54,810
also be a geometric
random variable.

81
00:03:54,810 --> 00:03:57,790
And because different slots
are independent, whatever

82
00:03:57,790 --> 00:04:00,580
happens after the first arrival
is independent from

83
00:04:00,580 --> 00:04:02,640
whatever happened before.

84
00:04:02,640 --> 00:04:06,250
So T1 and T2 will be independent
random variables.

85
00:04:06,250 --> 00:04:08,940
And similarly, all
the way up to Tk.

86
00:04:08,940 --> 00:04:13,460
So the time until the k-th
arrival is a sum of

87
00:04:13,460 --> 00:04:17,640
independent geometric random
variables, with the same

88
00:04:17,640 --> 00:04:19,459
parameter p.

89
00:04:19,459 --> 00:04:23,010
And we saw last time that we
can find the probability

90
00:04:23,010 --> 00:04:25,850
distribution of Yk.

91
00:04:25,850 --> 00:04:30,880
The probability that Yk takes
a value of t is equal to--

92
00:04:30,880 --> 00:04:36,110
there's this combinatorial
factor here, and then you get

93
00:04:36,110 --> 00:04:41,590
p to the k, (1-p) to the (t-k),
and this formula is

94
00:04:41,590 --> 00:04:48,020
true for t equal to
k, k+1, and so on.

95
00:04:48,020 --> 00:04:49,950
And this distribution
has a name.

96
00:04:49,950 --> 00:04:51,660
It's called the Pascal PMF.

97
00:04:54,500 --> 00:04:57,080
So this is all there
is to know about

98
00:04:57,080 --> 00:04:59,130
the Bernoulli process.

99
00:04:59,130 --> 00:05:02,880
One important comment is to
realize what exactly this

100
00:05:02,880 --> 00:05:05,540
memorylessness property
is saying.

101
00:05:05,540 --> 00:05:07,450
So I discussed it a little
bit last time.

102
00:05:07,450 --> 00:05:10,490
Let me reiterate it.

103
00:05:10,490 --> 00:05:13,460
So we have a Bernoulli process,
which is a sequence

104
00:05:13,460 --> 00:05:15,230
of Bernoulli trials.

105
00:05:15,230 --> 00:05:17,990
And these are (0,1) random
variables that

106
00:05:17,990 --> 00:05:20,320
keep going on forever.

107
00:05:20,320 --> 00:05:27,400
So someone is watching this
movie of Bernoulli trials B_t.

108
00:05:27,400 --> 00:05:31,540
And at some point, they say
they think, or something

109
00:05:31,540 --> 00:05:33,990
interesting has happened,
why don't you come

110
00:05:33,990 --> 00:05:36,300
in and start watching?

111
00:05:36,300 --> 00:05:39,760
So at some time t, they
tell you to come

112
00:05:39,760 --> 00:05:41,300
in and start watching.

113
00:05:41,300 --> 00:05:44,780
So what you will see once
you come in will

114
00:05:44,780 --> 00:05:48,320
be this future trials.

115
00:05:48,320 --> 00:05:52,610
So actually what you will see
is a random process, whose

116
00:05:52,610 --> 00:05:57,170
first random variable is going
to be the first one that you

117
00:05:57,170 --> 00:05:59,340
see, B_(t +1).

118
00:05:59,340 --> 00:06:03,040
The second one is going
to be this, and so on.

119
00:06:03,040 --> 00:06:06,850
So this is the process that's
seen by the person who's asked

120
00:06:06,850 --> 00:06:10,360
to come in and start watching
at that time.

121
00:06:10,360 --> 00:06:15,220
And the claim is that this
process is itself a Bernoulli

122
00:06:15,220 --> 00:06:20,380
process, provided that the
person who calls you into the

123
00:06:20,380 --> 00:06:23,740
room does not look
into the future.

124
00:06:23,740 --> 00:06:27,100
The person who calls you into
the room decides to call you

125
00:06:27,100 --> 00:06:31,180
in only on the basis of what
they have seen so far.

126
00:06:31,180 --> 00:06:33,860
So for example, who calls you
into the room might have a

127
00:06:33,860 --> 00:06:39,600
rule that says, as soon as I see
a sequence of 3 heads, I

128
00:06:39,600 --> 00:06:43,730
ask the other person
to come in.

129
00:06:43,730 --> 00:06:46,390
So if they use that particular
rule, it means that when

130
00:06:46,390 --> 00:06:49,900
you're called in, the previous
3 were heads.

131
00:06:49,900 --> 00:06:53,190
But this doesn't give you any
information about the future.

132
00:06:53,190 --> 00:06:55,260
And so the future ones
will be just

133
00:06:55,260 --> 00:06:57,160
independent Bernoulli trials.

134
00:06:57,160 --> 00:07:00,370
If on the other hand, the person
who calls you in has

135
00:07:00,370 --> 00:07:04,180
seen the movie before and they
use a rule, such as, for

136
00:07:04,180 --> 00:07:09,710
example, I call you in just
before 3 heads show up for the

137
00:07:09,710 --> 00:07:10,820
first time.

138
00:07:10,820 --> 00:07:13,850
So the person calls you in based
on knowledge that these

139
00:07:13,850 --> 00:07:15,430
two would be three heads.

140
00:07:15,430 --> 00:07:17,490
If they have such foresight--

141
00:07:17,490 --> 00:07:19,820
if they can look into
the future--

142
00:07:19,820 --> 00:07:25,050
then X1, X2, X3, they're certain
to be three heads, so

143
00:07:25,050 --> 00:07:27,190
they do not correspond
to random

144
00:07:27,190 --> 00:07:29,680
independent Bernoulli trials.

145
00:07:29,680 --> 00:07:33,660
So to rephrase this, the
process is memoryless.

146
00:07:33,660 --> 00:07:38,460
It does not matter what has
happened in the past.

147
00:07:38,460 --> 00:07:42,090
And that's true even if you are
called into the room and

148
00:07:42,090 --> 00:07:45,700
start watching at a random time,
as long as that random

149
00:07:45,700 --> 00:07:50,950
time is determined in a causal
way on the basis of what has

150
00:07:50,950 --> 00:07:52,340
happened so far.

151
00:07:52,340 --> 00:07:55,660
So you are called into the room
in a causal manner, just

152
00:07:55,660 --> 00:07:57,630
based on what's happened
so far.

153
00:07:57,630 --> 00:08:00,030
What you're going to see
starting from that time will

154
00:08:00,030 --> 00:08:03,240
still be a sequence of
independent Bernoulli trials.

155
00:08:03,240 --> 00:08:06,560
And this is the argument that we
used here, essentially, to

156
00:08:06,560 --> 00:08:09,190
argue that this T2 is an
independent random

157
00:08:09,190 --> 00:08:11,030
variable from T1.

158
00:08:11,030 --> 00:08:14,530
So a person is watching the
movie, sees the first success.

159
00:08:17,260 --> 00:08:19,810
And on the basis of what
they have seen--

160
00:08:19,810 --> 00:08:21,620
they have just seen the
first success--

161
00:08:21,620 --> 00:08:23,590
they ask you to come in.

162
00:08:23,590 --> 00:08:24,350
You come in.

163
00:08:24,350 --> 00:08:27,260
What you're going to see is a
sequence of Bernoulli trials.

164
00:08:27,260 --> 00:08:32,260
And you wait this long until
the next success comes in.

165
00:08:32,260 --> 00:08:35,390
What you see is a Bernoulli
process, as if the process was

166
00:08:35,390 --> 00:08:37,299
just starting right now.

167
00:08:37,299 --> 00:08:40,830
And that convinces us that this
should be a geometric

168
00:08:40,830 --> 00:08:43,200
random variable of the same
kind as this one, as

169
00:08:43,200 --> 00:08:47,080
independent from what
happened before.

170
00:08:47,080 --> 00:08:47,370
All right.

171
00:08:47,370 --> 00:08:49,610
So this is pretty much all there
is to know about the

172
00:08:49,610 --> 00:08:50,650
Bernoulli process.

173
00:08:50,650 --> 00:08:52,860
Plus the two things that we
did at the end of the last

174
00:08:52,860 --> 00:08:55,640
lecture where we merge two
independent Bernoulli

175
00:08:55,640 --> 00:08:58,070
processes, we get a
Bernoulli process.

176
00:08:58,070 --> 00:09:01,240
If we have a Bernoulli process
and we split it by flipping a

177
00:09:01,240 --> 00:09:05,450
coin and sending things one way
or the other, then we get

178
00:09:05,450 --> 00:09:07,790
two separate Bernoulli
processes.

179
00:09:07,790 --> 00:09:10,590
And we see that all of
these carry over to

180
00:09:10,590 --> 00:09:11,690
the continuous time.

181
00:09:11,690 --> 00:09:14,700
And our task for today is
basically to work these

182
00:09:14,700 --> 00:09:18,000
continuous time variations.

183
00:09:18,000 --> 00:09:21,440
So the Poisson process is a
continuous time version of the

184
00:09:21,440 --> 00:09:23,480
Bernoulli process.

185
00:09:23,480 --> 00:09:25,250
Here's the motivation
for considering

186
00:09:25,250 --> 00:09:26,930
it a Bernoulli process.

187
00:09:26,930 --> 00:09:29,850
So you have that person whose
job is to sit outside

188
00:09:29,850 --> 00:09:32,120
the door of a bank.

189
00:09:32,120 --> 00:09:38,280
And they have this long sheet,
and for every one second slot,

190
00:09:38,280 --> 00:09:42,560
they mark an X if a person
came in, or they mark

191
00:09:42,560 --> 00:09:45,760
something else if no one came
in during that slot.

192
00:09:45,760 --> 00:09:48,500
Now the bank manager is a really
scientifically trained

193
00:09:48,500 --> 00:09:50,530
person and wants very
accurate results.

194
00:09:50,530 --> 00:09:53,380
So they tell you, don't use
one second slots, use

195
00:09:53,380 --> 00:09:54,400
milliseconds slots.

196
00:09:54,400 --> 00:09:57,160
So you have all those slots
and you keep filling if

197
00:09:57,160 --> 00:09:59,950
someone arrived or not
during that slot.

198
00:09:59,950 --> 00:10:01,870
Well then you come
up with an idea.

199
00:10:01,870 --> 00:10:06,760
Why use millisecond slots and
keep putting crosses or zero's

200
00:10:06,760 --> 00:10:08,150
into each slot?

201
00:10:08,150 --> 00:10:12,380
It's much simpler if I just
record the exact times when

202
00:10:12,380 --> 00:10:14,010
people came in.

203
00:10:14,010 --> 00:10:16,380
So time is continuous.

204
00:10:16,380 --> 00:10:20,340
I don't keep doing something
at every time slot.

205
00:10:20,340 --> 00:10:24,440
But instead of the time axis,
I mark the times at which

206
00:10:24,440 --> 00:10:26,370
customers arrive.

207
00:10:26,370 --> 00:10:28,620
So there's no real
need for slots.

208
00:10:28,620 --> 00:10:32,370
The only information that you
want is when did we have

209
00:10:32,370 --> 00:10:34,130
arrivals of people.

210
00:10:34,130 --> 00:10:37,830
And we want to now model a
process of this kind happening

211
00:10:37,830 --> 00:10:41,880
in continuous time, that has the
same flavor, however, as

212
00:10:41,880 --> 00:10:44,210
the Bernoulli process.

213
00:10:44,210 --> 00:10:48,170
So that's the model we
want to develop.

214
00:10:48,170 --> 00:10:48,550
OK.

215
00:10:48,550 --> 00:10:52,880
So what are the properties
that we're going to have?

216
00:10:52,880 --> 00:10:57,190
First, we're going to assume
that intervals over the same

217
00:10:57,190 --> 00:11:01,340
length behave probabilistically
in an

218
00:11:01,340 --> 00:11:04,500
identical fashion.

219
00:11:04,500 --> 00:11:06,740
So what does that mean?

220
00:11:06,740 --> 00:11:09,640
Think of an interval of
some given length.

221
00:11:09,640 --> 00:11:12,450
During the interval of that
length, there's going to be a

222
00:11:12,450 --> 00:11:14,750
random number of arrivals.

223
00:11:14,750 --> 00:11:17,140
And that random number of
arrivals is going to have a

224
00:11:17,140 --> 00:11:19,050
probability distribution.

225
00:11:19,050 --> 00:11:20,630
So that probability
distribution--

226
00:11:20,630 --> 00:11:24,820
let's denote it by
this notation.

227
00:11:24,820 --> 00:11:29,410
We fix t, we fix the duration.

228
00:11:29,410 --> 00:11:31,650
So this is fixed.

229
00:11:31,650 --> 00:11:34,180
And we look at the
different k's.

230
00:11:34,180 --> 00:11:37,190
The probability of having 0
arrivals, the probability of 1

231
00:11:37,190 --> 00:11:40,100
arrival, the probability of
2 arrivals, and so on.

232
00:11:40,100 --> 00:11:42,630
So this thing is essentially
a PMF.

233
00:11:42,630 --> 00:11:46,920
So it should have the property
that the sum over all k's of

234
00:11:46,920 --> 00:11:49,890
this P_(k, tau) should
be equal to 1.

235
00:11:52,620 --> 00:11:57,490
Now, hidden inside this notation
is an assumption of

236
00:11:57,490 --> 00:11:59,550
time homogeneity.

237
00:11:59,550 --> 00:12:03,520
That is, this probability
distribution for the number of

238
00:12:03,520 --> 00:12:09,020
arrivals only depends on the
length of the interval, but

239
00:12:09,020 --> 00:12:13,230
not the exact location of the
interval on the time axis.

240
00:12:13,230 --> 00:12:18,810
That is, if I take an interval
of length tau, and I ask about

241
00:12:18,810 --> 00:12:21,200
the number of arrivals
in this interval.

242
00:12:21,200 --> 00:12:24,990
And I take another interval of
length tau, and I ask about

243
00:12:24,990 --> 00:12:27,790
the number of arrivals
during that interval.

244
00:12:27,790 --> 00:12:31,040
Number of arrivals here, and
number of arrivals there have

245
00:12:31,040 --> 00:12:34,400
the same probability
distribution, which is

246
00:12:34,400 --> 00:12:36,990
denoted this way.

247
00:12:36,990 --> 00:12:41,650
So the statistical behavior of
arrivals here is the same as

248
00:12:41,650 --> 00:12:44,310
the statistical behavioral
of arrivals there.

249
00:12:44,310 --> 00:12:46,820
What's the relation with
the Bernoulli process?

250
00:12:46,820 --> 00:12:48,510
It's very much like
the assumption--

251
00:12:48,510 --> 00:12:49,710
the Bernoulli process--

252
00:12:49,710 --> 00:12:52,530
that in different slots,
we have the same

253
00:12:52,530 --> 00:12:54,450
probability of success.

254
00:12:54,450 --> 00:12:56,770
Every slot looks
probabilistically

255
00:12:56,770 --> 00:12:58,400
as any other slot.

256
00:12:58,400 --> 00:13:02,760
So similarly here, any interval
of length tau looks

257
00:13:02,760 --> 00:13:06,790
probabilistically as any other
interval of length tau.

258
00:13:06,790 --> 00:13:09,710
And the number of arrivals
during that interval is a

259
00:13:09,710 --> 00:13:12,350
random variable described
by these probabilities.

260
00:13:12,350 --> 00:13:15,570
Number of arrivals here is a
random variable described by

261
00:13:15,570 --> 00:13:18,270
these same probabilities.

262
00:13:18,270 --> 00:13:19,900
So that's our first
assumption.

263
00:13:19,900 --> 00:13:21,030
Then what else?

264
00:13:21,030 --> 00:13:23,340
In the Bernoulli process we
had the assumption that

265
00:13:23,340 --> 00:13:27,710
different time slots were
independent of each other.

266
00:13:27,710 --> 00:13:32,970
Here we do not have time slots,
but we can still think

267
00:13:32,970 --> 00:13:37,490
in a similar way and impose the
following assumption, that

268
00:13:37,490 --> 00:13:40,970
these joint time intervals are
statistically independent.

269
00:13:40,970 --> 00:13:42,640
What does that mean?

270
00:13:42,640 --> 00:13:45,530
Does a random number of arrivals
during this interval,

271
00:13:45,530 --> 00:13:48,270
and the random number of
arrivals during this interval,

272
00:13:48,270 --> 00:13:49,830
and the random number of

273
00:13:49,830 --> 00:13:51,730
arrivals during this interval--

274
00:13:51,730 --> 00:13:55,110
so these are three different
random variables--

275
00:13:55,110 --> 00:13:58,900
these three random variables are
independent of each other.

276
00:13:58,900 --> 00:14:02,170
How many arrivals we got here
is independent from how many

277
00:14:02,170 --> 00:14:04,040
arrivals we got there.

278
00:14:04,040 --> 00:14:07,410
So this is similar to saying
that different time slots were

279
00:14:07,410 --> 00:14:08,110
independent.

280
00:14:08,110 --> 00:14:10,020
That's what we did
in discrete time.

281
00:14:10,020 --> 00:14:13,190
The continuous time analog is
this independence assumption.

282
00:14:13,190 --> 00:14:16,400
So for example, in particular,
number of arrivals here is

283
00:14:16,400 --> 00:14:19,440
independent from the number
of arrivals there.

284
00:14:19,440 --> 00:14:22,980
So these are two basic
assumptions about the process.

285
00:14:25,620 --> 00:14:30,270
Now in order to write down a
formula, eventually, about

286
00:14:30,270 --> 00:14:33,050
this probability
distribution--

287
00:14:33,050 --> 00:14:36,860
which is our next objective, we
would like to say something

288
00:14:36,860 --> 00:14:38,790
specific about this
distribution

289
00:14:38,790 --> 00:14:40,310
of number of arrivals--

290
00:14:40,310 --> 00:14:43,980
we need to add a little more
structure into the problem.

291
00:14:43,980 --> 00:14:47,380
And we're going to make the
following assumption.

292
00:14:47,380 --> 00:14:51,140
If we look at the time interval
of length delta--

293
00:14:51,140 --> 00:14:54,090
and delta now is supposed to
be a small number, so a

294
00:14:54,090 --> 00:14:55,900
picture like this--

295
00:14:55,900 --> 00:15:00,790
during a very small time
interval, there is a

296
00:15:00,790 --> 00:15:06,140
probability that we get exactly
one arrival, which is

297
00:15:06,140 --> 00:15:07,750
lambda times delta.

298
00:15:07,750 --> 00:15:10,630
Delta is the length of the
interval and lambda is a

299
00:15:10,630 --> 00:15:14,510
proportionality factor, which
is sort of the intensity of

300
00:15:14,510 --> 00:15:16,190
the arrival process.

301
00:15:16,190 --> 00:15:21,000
Bigger lambda means that a
little interval is more likely

302
00:15:21,000 --> 00:15:24,140
to get an arrival.

303
00:15:24,140 --> 00:15:25,595
So there's a probability lambda

304
00:15:25,595 --> 00:15:27,570
times delta of 1 arrival.

305
00:15:27,570 --> 00:15:31,740
The remaining probability
goes to 0 arrivals.

306
00:15:31,740 --> 00:15:35,560
And when delta is small, the
probability of 2 arrivals can

307
00:15:35,560 --> 00:15:39,660
be approximated by 0.

308
00:15:39,660 --> 00:15:42,460
So this is a description
of what happens during

309
00:15:42,460 --> 00:15:45,240
a small, tiny slot.

310
00:15:45,240 --> 00:15:48,200
Now this is something that's
supposed to be true in some

311
00:15:48,200 --> 00:15:51,530
limiting sense, when delta
is very small.

312
00:15:51,530 --> 00:15:56,070
So the exact version of this
statement would be that this

313
00:15:56,070 --> 00:16:02,050
is an equality, plus order
of delta squared terms.

314
00:16:02,050 --> 00:16:04,060
So this is an approximate
equality.

315
00:16:04,060 --> 00:16:07,850
And what approximation means is
that in the limit of small

316
00:16:07,850 --> 00:16:11,970
deltas, the dominant terms--

317
00:16:11,970 --> 00:16:15,900
the constant and the first order
term are given by this.

318
00:16:15,900 --> 00:16:19,760
Now when delta is very small,
second order terms in delta do

319
00:16:19,760 --> 00:16:21,130
not matter.

320
00:16:21,130 --> 00:16:24,380
They are small compared
to first order terms.

321
00:16:24,380 --> 00:16:26,190
So we ignore this.

322
00:16:26,190 --> 00:16:30,280
So you can either think in terms
of an exact relation,

323
00:16:30,280 --> 00:16:34,640
which is the probabilities are
given by this, plus delta

324
00:16:34,640 --> 00:16:35,990
squared terms.

325
00:16:35,990 --> 00:16:38,730
Or if you want to be a little
more loose, you just write

326
00:16:38,730 --> 00:16:41,000
here, as an approximate
equality.

327
00:16:41,000 --> 00:16:44,080
And the understanding is that
this equality holds--

328
00:16:44,080 --> 00:16:50,850
approximately becomes more
and more correct as

329
00:16:50,850 --> 00:16:53,410
delta goes to 0.

330
00:16:53,410 --> 00:16:57,250
So another version of that
statement would be that if you

331
00:16:57,250 --> 00:17:03,280
take the limit as delta goes to
0, of p, the probability of

332
00:17:03,280 --> 00:17:06,829
having 1 arrival in an interval
of length delta,

333
00:17:06,829 --> 00:17:10,300
divided by delta, this
is equal to lambda.

334
00:17:10,300 --> 00:17:16,010
So that would be one version of
an exact statement of what

335
00:17:16,010 --> 00:17:19,250
we are assuming here.

336
00:17:19,250 --> 00:17:22,750
So this lambda, we call it the
arrival rate, or the intensity

337
00:17:22,750 --> 00:17:23,930
of the process.

338
00:17:23,930 --> 00:17:27,349
And clearly, if you double
lambda, then a little interval

339
00:17:27,349 --> 00:17:29,340
is likely --

340
00:17:29,340 --> 00:17:31,630
you expect to get --

341
00:17:31,630 --> 00:17:34,200
the probability of obtaining
an arrival during that

342
00:17:34,200 --> 00:17:35,740
interval has doubled.

343
00:17:35,740 --> 00:17:40,400
So in some sense we have twice
as intense arrival process.

344
00:17:40,400 --> 00:17:46,470
If you look at the number
of arrivals during delta

345
00:17:46,470 --> 00:17:54,490
interval, what is the expected
value of that random variable?

346
00:17:54,490 --> 00:18:00,100
Well with probability lambda
delta we get 1 arrival.

347
00:18:00,100 --> 00:18:01,240
And with the remaining

348
00:18:01,240 --> 00:18:03,610
probability, we get 0 arrivals.

349
00:18:03,610 --> 00:18:06,680
So it's just lambda
times delta.

350
00:18:06,680 --> 00:18:10,640
So expected number of arrivals
during a little interval is

351
00:18:10,640 --> 00:18:12,170
lambda times delta.

352
00:18:12,170 --> 00:18:15,460
So expected number of arrivals
is proportional to lambda, and

353
00:18:15,460 --> 00:18:19,050
that's again why we call lambda
the arrival rate.

354
00:18:19,050 --> 00:18:22,820
If you send delta to the
denominator in this equality,

355
00:18:22,820 --> 00:18:26,380
it tells you that lambda is
the expected number of

356
00:18:26,380 --> 00:18:30,000
arrivals per unit time.

357
00:18:30,000 --> 00:18:37,010
So the arrival rate is expected
number of arrivals

358
00:18:37,010 --> 00:18:38,750
per unit time.

359
00:18:38,750 --> 00:18:42,580
And again, that justifies why
we call lambda the intensity

360
00:18:42,580 --> 00:18:43,830
of this process.

361
00:18:46,316 --> 00:18:46,760
All right.

362
00:18:46,760 --> 00:18:49,680
So where are we now?

363
00:18:49,680 --> 00:18:53,740
For the Bernoulli process, the
number of arrivals during a

364
00:18:53,740 --> 00:19:00,210
given interval of length n had
the PMF that we knew it was

365
00:19:00,210 --> 00:19:01,545
the binomial PMF.

366
00:19:04,190 --> 00:19:07,530
What is the formula for the
corresponding PMF for the

367
00:19:07,530 --> 00:19:09,190
continuous time process?

368
00:19:09,190 --> 00:19:12,570
Somehow we would like to use
our assumptions and come up

369
00:19:12,570 --> 00:19:16,100
with the formula for
this quantity.

370
00:19:16,100 --> 00:19:19,110
So this tells us about the
distribution of number of

371
00:19:19,110 --> 00:19:23,750
arrivals during an interval
of some general length.

372
00:19:23,750 --> 00:19:27,690
We have made assumptions about
the number of arrivals during

373
00:19:27,690 --> 00:19:30,410
an interval of small length.

374
00:19:30,410 --> 00:19:34,350
An interval of big length is
composed of many intervals of

375
00:19:34,350 --> 00:19:37,830
small length, so maybe this
is the way to go.

376
00:19:37,830 --> 00:19:43,120
Take a big interval, and split
it into many intervals of

377
00:19:43,120 --> 00:19:44,970
small length.

378
00:19:44,970 --> 00:19:48,410
So we have here our time axis.

379
00:19:48,410 --> 00:19:51,480
And we have an interval
of length tau.

380
00:19:51,480 --> 00:19:55,240
And I'm going to split it into
lots of little intervals of

381
00:19:55,240 --> 00:19:56,580
length delta.

382
00:19:56,580 --> 00:19:59,000
So how many intervals are
we going to have?

383
00:19:59,000 --> 00:20:03,060
The number of intervals is going
to be the total time,

384
00:20:03,060 --> 00:20:04,620
divided by delta.

385
00:20:07,520 --> 00:20:12,960
Now what happens during each one
of these little intervals?

386
00:20:12,960 --> 00:20:22,380
As long as the intervals are
small, what you have is that

387
00:20:22,380 --> 00:20:24,240
during an interval, you're
going to have

388
00:20:24,240 --> 00:20:27,220
either 0 or 1 arrival.

389
00:20:27,220 --> 00:20:29,940
The probability of more than
1 arrival during a little

390
00:20:29,940 --> 00:20:31,950
interval is negligible.

391
00:20:31,950 --> 00:20:35,380
So with this picture, you have
essentially a Bernoulli

392
00:20:35,380 --> 00:20:39,970
process that consists
of so many trials.

393
00:20:39,970 --> 00:20:43,160
And during each one of those
trials, we have a probability

394
00:20:43,160 --> 00:20:46,730
of success, which is
lambda times delta.

395
00:20:51,845 --> 00:20:54,330
Different little intervals
here are

396
00:20:54,330 --> 00:20:56,140
independent of each other.

397
00:20:56,140 --> 00:20:58,670
That's one of our assumptions,
that these joint time

398
00:20:58,670 --> 00:21:00,380
intervals are independent.

399
00:21:00,380 --> 00:21:05,590
So approximately, what we have
is a Bernoulli process.

400
00:21:05,590 --> 00:21:06,980
We have independence.

401
00:21:06,980 --> 00:21:09,250
We have the number of
slots of interest.

402
00:21:09,250 --> 00:21:11,450
And during each one of the
slots we have a certain

403
00:21:11,450 --> 00:21:13,530
probability of success.

404
00:21:13,530 --> 00:21:17,300
So if we think of this as
another good approximation of

405
00:21:17,300 --> 00:21:18,870
the Poisson process--

406
00:21:18,870 --> 00:21:21,090
with the approximation becoming
more and more

407
00:21:21,090 --> 00:21:23,595
accurate as delta goes to 0 --

408
00:21:23,595 --> 00:21:28,150
what we should do would be to
take the formula for the PMF

409
00:21:28,150 --> 00:21:32,320
of number of arrivals in a
Bernoulli process, and then

410
00:21:32,320 --> 00:21:37,230
take the limit as
delta goes to 0.

411
00:21:37,230 --> 00:21:45,260
So in the Bernoulli process, the
probability of k arrivals

412
00:21:45,260 --> 00:21:52,730
is n choose k, and then
you have p to the k.

413
00:21:52,730 --> 00:21:57,610
Now in our case, we have here
lambda times delta, delta is

414
00:21:57,610 --> 00:21:59,340
tau over n.

415
00:22:02,190 --> 00:22:08,410
Delta is tau over n, so p is
lambda times tau divided by n.

416
00:22:08,410 --> 00:22:11,010
So here's our p --

417
00:22:11,010 --> 00:22:13,690
Lambda tau over n --

418
00:22:13,690 --> 00:22:22,760
to the power k, and then times
one minus this-- this is our

419
00:22:22,760 --> 00:22:24,540
one minus p--

420
00:22:24,540 --> 00:22:25,790
to the power n-k.

421
00:22:30,010 --> 00:22:35,730
So this is the exact formula
for the Bernoulli process.

422
00:22:35,730 --> 00:22:39,830
For the Poisson process, what we
do is we take that formula

423
00:22:39,830 --> 00:22:43,360
and we let delta go to 0.

424
00:22:43,360 --> 00:22:48,150
As delta goes to 0, n
goes to infinity.

425
00:22:48,150 --> 00:22:51,280
So that's the limit
that we're taking.

426
00:22:51,280 --> 00:22:55,580
On the other hand, this
expression lambda times tau--

427
00:22:59,730 --> 00:23:03,740
lambda times tau, what
is it going to be?

428
00:23:03,740 --> 00:23:06,860
Lambda times tau is equal
to n times p.

429
00:23:09,900 --> 00:23:11,990
n times p, is that
what I want?

430
00:23:21,300 --> 00:23:22,550
No, let's see.

431
00:23:26,600 --> 00:23:28,110
Lambda tau is np.

432
00:23:28,110 --> 00:23:29,990
Yeah.

433
00:23:29,990 --> 00:23:32,060
So lambda tau is np.

434
00:23:53,370 --> 00:23:54,030
All right.

435
00:23:54,030 --> 00:23:59,320
So we have this relation,
lambda tau equals np.

436
00:23:59,320 --> 00:24:03,070
These two numbers being equal
kind of makes sense. np is the

437
00:24:03,070 --> 00:24:05,890
expected number of successes
you're going to get in the

438
00:24:05,890 --> 00:24:07,750
Bernoulli process.

439
00:24:07,750 --> 00:24:08,780
Lambda tau--

440
00:24:08,780 --> 00:24:11,800
since lambda is the arrival rate
and you have a total time

441
00:24:11,800 --> 00:24:15,710
of tau, lambda tau you can think
of it as the number of

442
00:24:15,710 --> 00:24:19,750
expected arrivals in the
Bernoulli process.

443
00:24:19,750 --> 00:24:22,000
We're doing a Bernoulli
approximation

444
00:24:22,000 --> 00:24:23,250
to the Poisson process.

445
00:24:23,250 --> 00:24:26,150
We take the formula for the
Bernoulli, and now take the

446
00:24:26,150 --> 00:24:30,060
limit as n goes to infinity.

447
00:24:30,060 --> 00:24:35,330
Now lambda tau over n is equal
to p, so it's clear what this

448
00:24:35,330 --> 00:24:37,040
term is going to give us.

449
00:24:37,040 --> 00:24:39,695
This is just p to the power k.

450
00:24:48,820 --> 00:24:53,230
It will actually take a little
more work than that.

451
00:24:53,230 --> 00:24:58,210
Now I'm not going to do the
algebra, but I'm just telling

452
00:24:58,210 --> 00:25:03,390
you that one can take the limit
in this formula here, as

453
00:25:03,390 --> 00:25:05,000
n goes to infinity.

454
00:25:05,000 --> 00:25:08,860
And that will give you another
formula, the final formula for

455
00:25:08,860 --> 00:25:10,320
the Poisson PMF.

456
00:25:10,320 --> 00:25:13,400
One thing to notice is that here
you have something like 1

457
00:25:13,400 --> 00:25:17,460
minus a constant over
n, to the power n.

458
00:25:17,460 --> 00:25:21,630
And you may recall from calculus
a formula of this

459
00:25:21,630 --> 00:25:26,540
kind, that this converges
to e to the minus c.

460
00:25:26,540 --> 00:25:29,560
If you remember that formula
from calculus, then you will

461
00:25:29,560 --> 00:25:32,750
expect that here, in the limit,
you are going to get

462
00:25:32,750 --> 00:25:36,520
something like an e to
the minus lambda tau.

463
00:25:36,520 --> 00:25:39,180
So indeed, we will
get such a term.

464
00:25:39,180 --> 00:25:42,230
There is some work that needs
to be done to find the limit

465
00:25:42,230 --> 00:25:45,880
of this expression, times
that expression.

466
00:25:45,880 --> 00:25:48,690
The algebra is not hard,
it's in the text.

467
00:25:48,690 --> 00:25:51,340
Let's not spend more
time doing this.

468
00:25:51,340 --> 00:25:53,820
But let me just give you
the formula of what

469
00:25:53,820 --> 00:25:55,620
comes at the end.

470
00:25:55,620 --> 00:25:59,720
And the formula that comes at
the end is of this form.

471
00:25:59,720 --> 00:26:03,710
So what matters here is not so
much the specific algebra that

472
00:26:03,710 --> 00:26:07,690
you will do to go from this
formula to that one.

473
00:26:07,690 --> 00:26:09,370
It's kind of straightforward.

474
00:26:09,370 --> 00:26:14,535
What's important is the idea
that the Poisson process, by

475
00:26:14,535 --> 00:26:19,710
definition, can be approximated
by a Bernoulli

476
00:26:19,710 --> 00:26:25,040
process in which we have a very
large number of slots--

477
00:26:25,040 --> 00:26:27,970
n goes to infinity.

478
00:26:27,970 --> 00:26:32,600
Whereas we have a very small
probability of success during

479
00:26:32,600 --> 00:26:34,420
each time slot.

480
00:26:34,420 --> 00:26:38,640
So a large number of slots,
but tiny probability of

481
00:26:38,640 --> 00:26:40,480
success during each slot.

482
00:26:40,480 --> 00:26:42,370
And we take the limit
as the slots

483
00:26:42,370 --> 00:26:44,680
become smaller and smaller.

484
00:26:44,680 --> 00:26:47,170
So with this approximation
we end up with

485
00:26:47,170 --> 00:26:49,030
this particular formula.

486
00:26:49,030 --> 00:26:51,890
And this is the so-called
Poisson PMF.

487
00:26:51,890 --> 00:26:53,930
Now this function P here --

488
00:26:53,930 --> 00:26:55,190
has two arguments.

489
00:26:55,190 --> 00:26:58,320
The important thing to realize
is that when you think of this

490
00:26:58,320 --> 00:27:02,900
as a PMF, you fix t to tau.

491
00:27:02,900 --> 00:27:06,010
And for a fixed tau,
now this is a PMF.

492
00:27:06,010 --> 00:27:11,260
As I said before, the sum over
k has to be equal to 1.

493
00:27:11,260 --> 00:27:15,510
So for a given tau, these
probabilities add up to 1.

494
00:27:15,510 --> 00:27:20,590
The formula is moderately messy,
but not too messy.

495
00:27:20,590 --> 00:27:24,570
One can work with it without
too much pain.

496
00:27:24,570 --> 00:27:28,460
And what's the mean and
variance of this PMF?

497
00:27:28,460 --> 00:27:31,560
Well what's the expected
number of arrivals?

498
00:27:31,560 --> 00:27:35,680
If you think of this Bernoulli
analogy, we know that the

499
00:27:35,680 --> 00:27:37,940
expected number of arrivals
in the Bernoulli

500
00:27:37,940 --> 00:27:41,250
process is n times p.

501
00:27:41,250 --> 00:27:44,500
In the approximation that
we're using in these

502
00:27:44,500 --> 00:27:48,170
procedure, n times p is the
same as lambda tau.

503
00:27:48,170 --> 00:27:52,490
And that's why we get lambda tau
to be the expected number

504
00:27:52,490 --> 00:27:53,480
of arrivals.

505
00:27:53,480 --> 00:27:56,140
Here I'm using t
instead of tau.

506
00:27:56,140 --> 00:28:01,670
The expected number of
arrivals is lambda t.

507
00:28:01,670 --> 00:28:05,450
So if you double the time,
you expect to get

508
00:28:05,450 --> 00:28:07,290
twice as many arrivals.

509
00:28:07,290 --> 00:28:10,960
If you double the arrival rate,
you expect to get twice

510
00:28:10,960 --> 00:28:12,760
as many arrivals.

511
00:28:12,760 --> 00:28:15,290
How about the formula
for the variance?

512
00:28:15,290 --> 00:28:18,930
The variance of the Bernoulli
process is np,

513
00:28:18,930 --> 00:28:22,720
times one minus p.

514
00:28:22,720 --> 00:28:25,800
What does this go
to in the limit?

515
00:28:25,800 --> 00:28:31,170
In the limit that we're taking,
as delta goes to zero,

516
00:28:31,170 --> 00:28:33,550
then p also goes to zero.

517
00:28:33,550 --> 00:28:37,270
The probability of success in
any given slot goes to zero.

518
00:28:37,270 --> 00:28:39,700
So this term becomes
insignificant.

519
00:28:39,700 --> 00:28:45,980
So this becomes n times p, which
is again lambda t, or

520
00:28:45,980 --> 00:28:47,710
lambda tau.

521
00:28:47,710 --> 00:28:50,960
So the variance, instead of
having this more complicated

522
00:28:50,960 --> 00:28:54,290
formula of the variance is the
Bernoulli process, here it

523
00:28:54,290 --> 00:28:56,840
gets simplified and
it's lambda t.

524
00:28:56,840 --> 00:29:00,580
So interestingly, the variance
in the Poisson process is

525
00:29:00,580 --> 00:29:03,200
exactly the same as the
expected value.

526
00:29:03,200 --> 00:29:06,360
So you can look at this as
just some interesting

527
00:29:06,360 --> 00:29:07,780
coincidence.

528
00:29:07,780 --> 00:29:10,260
So now we're going to take
this formula and

529
00:29:10,260 --> 00:29:11,370
see how to use it.

530
00:29:11,370 --> 00:29:14,060
First we're going to do
a completely trivial,

531
00:29:14,060 --> 00:29:16,560
straightforward example.

532
00:29:16,560 --> 00:29:24,630
So 15 years ago when that
example was made, email was

533
00:29:24,630 --> 00:29:27,230
coming at a rate of five
messages per hour.

534
00:29:27,230 --> 00:29:30,660
I wish that was the
case today.

535
00:29:30,660 --> 00:29:38,450
And now emails that are coming
in, let's say during the day--

536
00:29:38,450 --> 00:29:41,750
the arrival rates of emails
are probably different in

537
00:29:41,750 --> 00:29:42,960
different times of the day.

538
00:29:42,960 --> 00:29:46,840
But if you fix a time slot,
let's say 1:00 to 2:00 in the

539
00:29:46,840 --> 00:29:49,370
afternoon, there's probably
a constant rate.

540
00:29:49,370 --> 00:29:53,050
And email arrivals are
reasonably well modeled by a

541
00:29:53,050 --> 00:29:54,790
Poisson process.

542
00:29:54,790 --> 00:29:58,220
Speaking of modeling, it's
not just email arrivals.

543
00:29:58,220 --> 00:30:02,290
Whenever arrivals happen in a
completely random way, without

544
00:30:02,290 --> 00:30:05,370
any additional structure, the
Poisson process is a good

545
00:30:05,370 --> 00:30:07,010
model of these arrivals.

546
00:30:07,010 --> 00:30:10,200
So the times at which car
accidents will happen, that's

547
00:30:10,200 --> 00:30:11,450
a Poisson processes.

548
00:30:15,530 --> 00:30:19,550
If you have a very, very weak
light source that's shooting

549
00:30:19,550 --> 00:30:24,290
out photons, just one at a time,
the times at which these

550
00:30:24,290 --> 00:30:27,240
photons will go out is
well modeled again

551
00:30:27,240 --> 00:30:28,670
by a Poisson process.

552
00:30:28,670 --> 00:30:30,540
So it's completely random.

553
00:30:30,540 --> 00:30:35,230
Or if you have a radioactive
material where one atom at a

554
00:30:35,230 --> 00:30:43,720
time changes at random times.

555
00:30:43,720 --> 00:30:45,920
So it's a very slow
radioactive decay.

556
00:30:45,920 --> 00:30:48,900
The time at which these alpha
particles, or whatever we get

557
00:30:48,900 --> 00:30:51,580
emitted, again is going
to be described

558
00:30:51,580 --> 00:30:53,200
by a Poisson process.

559
00:30:53,200 --> 00:30:58,220
So if you have arrivals, or
emissions, that happen at

560
00:30:58,220 --> 00:31:02,660
completely random times, and
once in a while you get an

561
00:31:02,660 --> 00:31:07,500
arrival or an event, then the
Poisson process is a very good

562
00:31:07,500 --> 00:31:10,070
model for these events.

563
00:31:10,070 --> 00:31:12,200
So back to emails.

564
00:31:12,200 --> 00:31:16,350
Get them at a rate of five
messages per day, per hour.

565
00:31:16,350 --> 00:31:19,420
In 30 minutes this
is half an hour.

566
00:31:19,420 --> 00:31:23,770
So what we have is that
lambda t, total

567
00:31:23,770 --> 00:31:26,520
number of arrivals is--

568
00:31:26,520 --> 00:31:29,020
the expected number
of arrivals is--

569
00:31:29,020 --> 00:31:33,810
lambda is five, t is one-half,
if we talk about hours.

570
00:31:33,810 --> 00:31:36,480
So lambda t is two to the 0.5.

571
00:31:36,480 --> 00:31:41,220
The probability of no new
messages is the probability of

572
00:31:41,220 --> 00:31:48,560
zero, in time interval of length
t, which, in our case,

573
00:31:48,560 --> 00:31:51,790
is one-half.

574
00:31:51,790 --> 00:31:55,510
And then we look back into the
formula from the previous

575
00:31:55,510 --> 00:31:59,550
slide, and the probability of
zero arrivals is lambda t to

576
00:31:59,550 --> 00:32:03,770
the power zero, divided by zero
factorial, and then an e

577
00:32:03,770 --> 00:32:05,450
to the lambda t.

578
00:32:05,450 --> 00:32:07,840
And you plug in the numbers
that we have.

579
00:32:07,840 --> 00:32:10,380
Lambda t to the zero
power is one.

580
00:32:10,380 --> 00:32:12,040
Zero factorial is one.

581
00:32:12,040 --> 00:32:15,500
So we're left with e
to the minus 2.5.

582
00:32:15,500 --> 00:32:18,860
And that number is 0.08.

583
00:32:18,860 --> 00:32:22,090
Similarly, you can ask for the
probability that you get

584
00:32:22,090 --> 00:32:24,850
exactly one message
in half an hour.

585
00:32:24,850 --> 00:32:27,420
And that would be-- the
probability of one message in

586
00:32:27,420 --> 00:32:28,680
one-half an hour--

587
00:32:28,680 --> 00:32:32,590
is going to be lambda t to the
first power, divided by 1

588
00:32:32,590 --> 00:32:38,230
factorial, e to the minus
lambda t, which--

589
00:32:38,230 --> 00:32:41,900
as we now get the extra lambda t
factor-- is going to be 2.5,

590
00:32:41,900 --> 00:32:43,650
e to the minus 2.5.

591
00:32:43,650 --> 00:32:46,930
And the numerical
answer is 0.20.

592
00:32:46,930 --> 00:32:50,450
So this is how you use the PMF
formula for the Poisson

593
00:32:50,450 --> 00:32:55,540
distribution that we had
in the previous slide.

594
00:32:55,540 --> 00:32:55,890
All right.

595
00:32:55,890 --> 00:33:00,010
So this was all about
the distribution of

596
00:33:00,010 --> 00:33:01,780
the number of arrivals.

597
00:33:01,780 --> 00:33:03,350
What else did we do last time?

598
00:33:03,350 --> 00:33:08,250
Last time we also talked about
the time it takes until the

599
00:33:08,250 --> 00:33:09,500
k-th arrival.

600
00:33:12,390 --> 00:33:12,790
OK.

601
00:33:12,790 --> 00:33:16,020
So let's try to figure out
something about this

602
00:33:16,020 --> 00:33:18,260
particular distribution.

603
00:33:18,260 --> 00:33:21,180
We can derive the distribution
of the time of the k-th

604
00:33:21,180 --> 00:33:24,730
arrival by using the
exact same argument

605
00:33:24,730 --> 00:33:27,360
as we did last time.

606
00:33:27,360 --> 00:33:31,650
So now the time of the
k-th arrival is a

607
00:33:31,650 --> 00:33:33,830
continuous random variable.

608
00:33:33,830 --> 00:33:36,160
So it has a PDF.

609
00:33:36,160 --> 00:33:38,430
Since we are in continuous
time, arrivals can

610
00:33:38,430 --> 00:33:39,900
happen at any time.

611
00:33:39,900 --> 00:33:42,310
So Yk is a continuous
random variable.

612
00:33:45,200 --> 00:33:48,160
But now let's think of
a time interval of

613
00:33:48,160 --> 00:33:49,410
length little delta.

614
00:33:52,370 --> 00:33:58,620
And use our usual interpretation
of PDFs.

615
00:33:58,620 --> 00:34:03,180
The PDF of a random variable
evaluated at a certain time

616
00:34:03,180 --> 00:34:08,010
times delta, this is the
probability that the Yk falls

617
00:34:08,010 --> 00:34:09,514
in this little interval.

618
00:34:13,460 --> 00:34:16,639
So as I've said before, this
is the best way of thinking

619
00:34:16,639 --> 00:34:18,420
about PDFs.

620
00:34:18,420 --> 00:34:22,179
PDFs give you probabilities
of little intervals.

621
00:34:22,179 --> 00:34:25,540
So now let's try to calculate
this probability.

622
00:34:25,540 --> 00:34:29,880
For the k-th arrival to happen
inside this little interval,

623
00:34:29,880 --> 00:34:31,550
we need two things.

624
00:34:31,550 --> 00:34:35,790
We need an arrival to happen in
this interval, and we need

625
00:34:35,790 --> 00:34:41,530
k minus one arrivals to happen
during that interval.

626
00:34:41,530 --> 00:34:41,880
OK.

627
00:34:41,880 --> 00:34:45,469
You'll tell me, but it's
possible that we might have

628
00:34:45,469 --> 00:34:50,130
the k minus one arrival happen
here, and the k-th arrival to

629
00:34:50,130 --> 00:34:51,219
happen here.

630
00:34:51,219 --> 00:34:53,050
In principle, that's possible.

631
00:34:53,050 --> 00:34:56,139
But in the limit, when we take
delta very small, the

632
00:34:56,139 --> 00:34:59,850
probability of having two
arrivals in the same little

633
00:34:59,850 --> 00:35:01,830
slot is negligible.

634
00:35:01,830 --> 00:35:06,870
So assuming that no two arrivals
can happen in the

635
00:35:06,870 --> 00:35:10,940
same mini slot, then for the
k-th one to happen here, we

636
00:35:10,940 --> 00:35:15,710
must have k minus one during
this interval.

637
00:35:15,710 --> 00:35:20,210
Now because we have assumed that
these joint intervals are

638
00:35:20,210 --> 00:35:23,900
independent of each other,
this breaks down into the

639
00:35:23,900 --> 00:35:33,070
probability that we have exactly
k minus one arrivals,

640
00:35:33,070 --> 00:35:37,600
during the interval from zero to
t, times the probability of

641
00:35:37,600 --> 00:35:41,410
exactly one arrival during that
little interval, which is

642
00:35:41,410 --> 00:35:43,420
lambda delta.

643
00:35:43,420 --> 00:35:51,010
We do have a formula for this
from the previous slide, which

644
00:35:51,010 --> 00:35:59,340
is lambda t, to the k minus 1,
over k minus one factorial,

645
00:35:59,340 --> 00:36:07,190
times e to minus lambda t.

646
00:36:07,190 --> 00:36:09,070
And then lambda times delta.

647
00:36:14,910 --> 00:36:16,160
Did I miss something?

648
00:36:24,680 --> 00:36:26,310
Yeah, OK.

649
00:36:26,310 --> 00:36:26,970
All right.

650
00:36:26,970 --> 00:36:30,220
And now you cancel this
delta with that delta.

651
00:36:30,220 --> 00:36:36,820
And that gives us a formula for
the PDF of the time until

652
00:36:36,820 --> 00:36:39,170
the k-th arrival.

653
00:36:39,170 --> 00:36:43,290
This PDF, of course, depends
on the number k.

654
00:36:43,290 --> 00:36:46,850
The first arrival is going
to happen somewhere in

655
00:36:46,850 --> 00:36:48,040
this range of time.

656
00:36:48,040 --> 00:36:50,140
So this is the PDF
that it has.

657
00:36:50,140 --> 00:36:53,170
The second arrival, of course,
is going to happen later.

658
00:36:53,170 --> 00:36:54,860
And the PDF is this.

659
00:36:54,860 --> 00:36:57,880
So it's more likely to happen
around these times.

660
00:36:57,880 --> 00:37:01,410
The third arrival has this PDF,
so it's more likely to

661
00:37:01,410 --> 00:37:03,690
happen around those times.

662
00:37:03,690 --> 00:37:08,020
And if you were to take
k equal to 100,

663
00:37:08,020 --> 00:37:10,470
you might get a PDF--

664
00:37:10,470 --> 00:37:13,260
it's extremely unlikely that
the k-th arrival happens in

665
00:37:13,260 --> 00:37:18,060
the beginning, and it might
happen somewhere down there,

666
00:37:18,060 --> 00:37:20,010
far into the future.

667
00:37:20,010 --> 00:37:22,230
So depending on which particular
arrival we're

668
00:37:22,230 --> 00:37:25,510
talking about, it has a
different probability

669
00:37:25,510 --> 00:37:26,350
distribution.

670
00:37:26,350 --> 00:37:30,340
The time of the 100th arrival,
of course, is expected to be a

671
00:37:30,340 --> 00:37:34,100
lot larger than the time
of the first arrival.

672
00:37:34,100 --> 00:37:38,550
Incidentally, the time of the
first arrival has a PDF whose

673
00:37:38,550 --> 00:37:40,160
form is quite simple.

674
00:37:40,160 --> 00:37:43,850
If you let k equal to one here,
this term disappears.

675
00:37:43,850 --> 00:37:46,310
That term becomes a one.

676
00:37:46,310 --> 00:37:49,880
You're left with just lambda,
e to the minus lambda.

677
00:37:49,880 --> 00:37:53,210
And you recognize it, it's the
exponential distribution.

678
00:37:53,210 --> 00:37:57,210
So the time until the first
arrival in a Poisson process

679
00:37:57,210 --> 00:38:00,160
is an exponential
distribution.

680
00:38:00,160 --> 00:38:02,150
What was the time of the
first arrival in

681
00:38:02,150 --> 00:38:03,970
the Bernoulli process?

682
00:38:03,970 --> 00:38:07,060
It was a geometric
distribution.

683
00:38:07,060 --> 00:38:11,170
Well, not coincidentally, these
two look quite a bit

684
00:38:11,170 --> 00:38:13,030
like the other.

685
00:38:13,030 --> 00:38:17,980
A geometric distribution
has this kind of shape.

686
00:38:17,980 --> 00:38:21,900
The exponential distribution
has that kind of shape.

687
00:38:21,900 --> 00:38:25,560
The geometric is just a discrete
version of the

688
00:38:25,560 --> 00:38:27,090
exponential.

689
00:38:27,090 --> 00:38:29,860
In the Bernoulli case, we
are in discrete time.

690
00:38:29,860 --> 00:38:32,540
We have a PMF for the
time of the first

691
00:38:32,540 --> 00:38:35,080
arrival, which is geometric.

692
00:38:35,080 --> 00:38:38,540
In the Poisson case, what we
get is the limit of the

693
00:38:38,540 --> 00:38:41,560
geometric as you let those
lines become closer and

694
00:38:41,560 --> 00:38:46,480
closer, which gives you the
exponential distribution.

695
00:38:46,480 --> 00:38:50,430
Now the Poisson process shares
all the memorylessness

696
00:38:50,430 --> 00:38:52,870
properties of the Bernoulli
process.

697
00:38:52,870 --> 00:38:56,750
And the way one can argue is
just in terms of this picture.

698
00:38:56,750 --> 00:39:00,250
Since the Poisson process is
the limit of Bernoulli

699
00:39:00,250 --> 00:39:03,570
processes, whatever qualitative
processes you have

700
00:39:03,570 --> 00:39:07,340
in the Bernoulli process
remain valid

701
00:39:07,340 --> 00:39:08,360
for the Poisson process.

702
00:39:08,360 --> 00:39:11,470
In particular we have this
memorylessness property.

703
00:39:11,470 --> 00:39:15,120
You let the Poisson process run
for some time, and then

704
00:39:15,120 --> 00:39:16,600
you start watching it.

705
00:39:16,600 --> 00:39:18,520
What ever happened in
the past has no

706
00:39:18,520 --> 00:39:20,220
bearing about the future.

707
00:39:20,220 --> 00:39:23,150
Starting from right now, what's
going to happen in the

708
00:39:23,150 --> 00:39:27,330
future is described again by a
Poisson process, in the sense

709
00:39:27,330 --> 00:39:30,530
that during every little slot of
length delta, there's going

710
00:39:30,530 --> 00:39:33,790
to be a probability of lambda
delta of having an arrival.

711
00:39:33,790 --> 00:39:36,590
And that probably lambda
delta is the same-- is

712
00:39:36,590 --> 00:39:38,070
always lambda delta--

713
00:39:38,070 --> 00:39:41,270
no matter what happened in
the past of the process.

714
00:39:41,270 --> 00:39:47,040
And in particular, we could use
this argument to say that

715
00:39:47,040 --> 00:39:50,460
the time until the k-th arrival
is the time that it

716
00:39:50,460 --> 00:39:53,720
takes for the first
arrival to happen.

717
00:39:53,720 --> 00:39:56,380
OK, let me do it for
k equal to two.

718
00:39:56,380 --> 00:39:59,630
And then after the first arrival
happens, you wait a

719
00:39:59,630 --> 00:40:02,600
certain amount of time until
the second arrival happens.

720
00:40:02,600 --> 00:40:06,410
Now once the first arrival
happened, that's in the past.

721
00:40:06,410 --> 00:40:07,400
You start watching.

722
00:40:07,400 --> 00:40:10,690
From now on you have mini slots
of length delta, each

723
00:40:10,690 --> 00:40:13,230
one having a probability of
success lambda delta.

724
00:40:13,230 --> 00:40:16,230
It's as if we started the
Poisson process from scratch.

725
00:40:16,230 --> 00:40:19,310
So starting from that time,
the time until the next

726
00:40:19,310 --> 00:40:22,840
arrival is going to be again an
exponential distribution,

727
00:40:22,840 --> 00:40:26,010
which doesn't care about what
happened in the past, how long

728
00:40:26,010 --> 00:40:28,000
it took you for the
first arrival.

729
00:40:28,000 --> 00:40:33,410
So these two random variables
are going to be independent

730
00:40:33,410 --> 00:40:38,140
and exponential, with the
same parameter lambda.

731
00:40:38,140 --> 00:40:42,570
So among other things, what we
have done here is we have

732
00:40:42,570 --> 00:40:48,130
essentially derived the PDF of
the sum of k independent

733
00:40:48,130 --> 00:40:49,320
exponentials.

734
00:40:49,320 --> 00:40:53,990
The time of the k-th arrival
is the sum of k

735
00:40:53,990 --> 00:40:56,230
inter-arrival times.

736
00:40:56,230 --> 00:40:59,380
The inter-arrival times are all
independent of each other

737
00:40:59,380 --> 00:41:01,450
because of memorylessness.

738
00:41:01,450 --> 00:41:04,245
And they all have the same
exponential distribution.

739
00:41:07,130 --> 00:41:08,980
And by the way, this
gives you a way to

740
00:41:08,980 --> 00:41:11,080
simulate the Poisson process.

741
00:41:11,080 --> 00:41:14,070
If you wanted to simulate it
on your computer, you would

742
00:41:14,070 --> 00:41:20,140
have one option to break time
into tiny, tiny slots.

743
00:41:20,140 --> 00:41:24,030
And for every tiny slot, use
your random number generator

744
00:41:24,030 --> 00:41:27,520
to decide whether there
was an arrival or not.

745
00:41:27,520 --> 00:41:29,810
To get it very accurate,
you would have to

746
00:41:29,810 --> 00:41:32,090
use tiny, tiny slots.

747
00:41:32,090 --> 00:41:35,280
So that would be a lot
of computation.

748
00:41:35,280 --> 00:41:38,530
The more clever way of
simulating the Poisson process

749
00:41:38,530 --> 00:41:42,320
is you use your random number
generator to generate a sample

750
00:41:42,320 --> 00:41:45,280
from an exponential distribution
and call that

751
00:41:45,280 --> 00:41:47,240
your first arrival time.

752
00:41:47,240 --> 00:41:50,050
Then go back to the random
number generator, generate

753
00:41:50,050 --> 00:41:53,040
another independent sample,
again from the same

754
00:41:53,040 --> 00:41:54,780
exponential distribution.

755
00:41:54,780 --> 00:41:58,490
That's the time between the
first and the second arrival,

756
00:41:58,490 --> 00:42:01,390
and you keep going that way.

757
00:42:01,390 --> 00:42:03,260
So as a sort of a
quick summary,

758
00:42:03,260 --> 00:42:04,910
this is the big picture.

759
00:42:04,910 --> 00:42:08,630
This table doesn't tell
you anything new.

760
00:42:08,630 --> 00:42:12,230
But it's good to have it as a
reference, and to look at it,

761
00:42:12,230 --> 00:42:14,740
and to make sure you understand
what all the

762
00:42:14,740 --> 00:42:16,300
different boxes are.

763
00:42:16,300 --> 00:42:18,930
Basically the Bernoulli process
runs in discrete time.

764
00:42:18,930 --> 00:42:20,960
The Poisson process runs
in continuous time.

765
00:42:20,960 --> 00:42:25,140
There's an analogy of arrival
rates, p per trial, or

766
00:42:25,140 --> 00:42:27,270
intensity per unit time.

767
00:42:27,270 --> 00:42:32,190
We did derive, or sketched the
derivation for the PMF of the

768
00:42:32,190 --> 00:42:33,610
number of arrivals.

769
00:42:33,610 --> 00:42:37,810
And the Poisson distribution,
which is the distribution that

770
00:42:37,810 --> 00:42:40,450
we get, this Pk of t.

771
00:42:40,450 --> 00:42:44,220
Pk and t is the limit of the
binomial when we take the

772
00:42:44,220 --> 00:42:49,710
limit in this particular way,
as delta goes to zero, and n

773
00:42:49,710 --> 00:42:51,220
goes to infinity.

774
00:42:51,220 --> 00:42:54,270
The geometric becomes an
exponential in the limit.

775
00:42:54,270 --> 00:42:56,960
And the distribution of the
time of the k-th arrival--

776
00:42:56,960 --> 00:42:59,600
we had a closed form formula
last time for

777
00:42:59,600 --> 00:43:01,050
the Bernoulli process.

778
00:43:01,050 --> 00:43:03,930
We got the closed form
formula this time

779
00:43:03,930 --> 00:43:05,230
for the Poisson process.

780
00:43:05,230 --> 00:43:08,940
And we actually used exactly the
same argument to get these

781
00:43:08,940 --> 00:43:12,320
two closed form formulas.

782
00:43:12,320 --> 00:43:12,650
All right.

783
00:43:12,650 --> 00:43:18,280
So now let's talk about adding
or merging Poisson processes.

784
00:43:18,280 --> 00:43:21,060
And there's two statements
that we can make here.

785
00:43:21,060 --> 00:43:25,970
One has to do with adding
Poisson random variables, just

786
00:43:25,970 --> 00:43:26,820
random variables.

787
00:43:26,820 --> 00:43:28,290
There's another statement about

788
00:43:28,290 --> 00:43:30,770
adding Poisson processes.

789
00:43:30,770 --> 00:43:34,540
And the second is a bigger
statement than the first.

790
00:43:34,540 --> 00:43:36,140
But this is a warm up.

791
00:43:36,140 --> 00:43:39,140
Let's work with the
first statement.

792
00:43:39,140 --> 00:43:42,460
So the claim is that the sum of
independent Poisson random

793
00:43:42,460 --> 00:43:45,340
variables is Poisson.

794
00:43:45,340 --> 00:43:45,990
OK.

795
00:43:45,990 --> 00:43:50,490
So suppose that we have a
Poisson process with rate--

796
00:43:50,490 --> 00:43:51,760
just for simplicity--

797
00:43:51,760 --> 00:43:53,170
lambda one.

798
00:43:53,170 --> 00:43:56,240
And I take the interval
from zero to two.

799
00:43:56,240 --> 00:44:00,620
And that take then the interval
from two until five.

800
00:44:00,620 --> 00:44:03,720
The number of arrivals during
this interval--

801
00:44:03,720 --> 00:44:06,730
let's call it n from
zero to two--

802
00:44:06,730 --> 00:44:13,920
is going to be a Poisson
random variable, with

803
00:44:13,920 --> 00:44:18,240
parameter, or with mean, two.

804
00:44:18,240 --> 00:44:24,340
The number of arrivals during
this interval is n from time

805
00:44:24,340 --> 00:44:26,340
two until five.

806
00:44:26,340 --> 00:44:31,120
This is again a Poisson random
variable with mean equal to

807
00:44:31,120 --> 00:44:34,690
three, because the arrival rate
is 1 and the duration of

808
00:44:34,690 --> 00:44:36,990
the interval is three.

809
00:44:36,990 --> 00:44:41,320
These two random variables
are independent.

810
00:44:41,320 --> 00:44:43,760
They obey the Poisson
distribution

811
00:44:43,760 --> 00:44:45,640
that we derived before.

812
00:44:45,640 --> 00:44:50,930
If you add them, what you get
is the number of arrivals

813
00:44:50,930 --> 00:44:53,850
during the interval
from zero to five.

814
00:44:53,850 --> 00:44:56,290
Now what kind of distribution
does this

815
00:44:56,290 --> 00:44:57,910
random variable have?

816
00:44:57,910 --> 00:45:00,760
Well this is the number of
arrivals over an interval of a

817
00:45:00,760 --> 00:45:03,600
certain length in a
Poisson process.

818
00:45:03,600 --> 00:45:08,580
Therefore, this is also Poisson
with mean five.

819
00:45:16,520 --> 00:45:19,040
Because for the Poisson process
we know that this

820
00:45:19,040 --> 00:45:23,300
number of arrivals is Poisson,
this is Poisson, but also the

821
00:45:23,300 --> 00:45:26,610
number of overall arrivals
is also Poisson.

822
00:45:26,610 --> 00:45:30,040
This establishes that the sum
of a Poisson plus a Poisson

823
00:45:30,040 --> 00:45:32,200
random variable gives
us another

824
00:45:32,200 --> 00:45:33,630
Poisson random variable.

825
00:45:33,630 --> 00:45:37,110
So adding Poisson random
variables gives us a Poisson

826
00:45:37,110 --> 00:45:38,720
random variable.

827
00:45:38,720 --> 00:45:42,660
But now I'm going to make a more
general statement that

828
00:45:42,660 --> 00:45:44,940
it's not just number
of arrivals during

829
00:45:44,940 --> 00:45:46,415
a fixed time interval--

830
00:45:50,420 --> 00:45:53,240
it's not just numbers of
arrivals for given time

831
00:45:53,240 --> 00:45:54,260
intervals--

832
00:45:54,260 --> 00:45:57,770
but rather if you take two
different Poisson processes

833
00:45:57,770 --> 00:46:02,330
and add them up, the process
itself is Poisson in the sense

834
00:46:02,330 --> 00:46:05,930
that this process is going to
satisfy all the assumptions of

835
00:46:05,930 --> 00:46:07,510
a Poisson process.

836
00:46:07,510 --> 00:46:11,060
So the story is that you have
a red bulb that flashes at

837
00:46:11,060 --> 00:46:13,350
random times at the rate
of lambda one.

838
00:46:13,350 --> 00:46:14,980
It's a Poisson process.

839
00:46:14,980 --> 00:46:19,080
You have an independent process
where a green bulb

840
00:46:19,080 --> 00:46:21,230
flashes at random times.

841
00:46:21,230 --> 00:46:24,800
And you happen to be color
blind, so you just see when

842
00:46:24,800 --> 00:46:26,630
something is flashing.

843
00:46:26,630 --> 00:46:29,920
So these two are assumed to be
independent Poisson processes.

844
00:46:29,920 --> 00:46:34,968
What can we say about the
process that you observe?

845
00:46:34,968 --> 00:46:40,250
So in the processes that you
observe, if you take a typical

846
00:46:40,250 --> 00:46:45,170
time interval of length little
delta, what can happen during

847
00:46:45,170 --> 00:46:48,380
that little time interval?

848
00:46:48,380 --> 00:46:55,280
The red process may have
something flashing.

849
00:46:55,280 --> 00:46:56,815
So red flashes.

850
00:46:59,850 --> 00:47:01,580
Or the red does not.

851
00:47:06,610 --> 00:47:12,170
And for the other bulb, the
green bulb, there's two

852
00:47:12,170 --> 00:47:13,020
possibilities.

853
00:47:13,020 --> 00:47:17,910
The green one flashes.

854
00:47:17,910 --> 00:47:20,900
And the other possibility is
that the green does not.

855
00:47:24,990 --> 00:47:25,330
OK.

856
00:47:25,330 --> 00:47:29,070
So there's four possibilities
about what can happen during a

857
00:47:29,070 --> 00:47:31,170
little slot.

858
00:47:31,170 --> 00:47:36,080
The probability that the red one
flashes and the green one

859
00:47:36,080 --> 00:47:39,750
flashes, what is this
probability?

860
00:47:39,750 --> 00:47:43,510
It's lambda one delta that the
first one flashes, and lambda

861
00:47:43,510 --> 00:47:47,290
two delta that the
second one does.

862
00:47:47,290 --> 00:47:50,280
I'm multiplying probabilities
here because I'm making the

863
00:47:50,280 --> 00:47:52,645
assumption that the two
processes are independent.

864
00:47:55,330 --> 00:47:57,330
OK.

865
00:47:57,330 --> 00:48:00,130
Now the probability that
the red one flashes

866
00:48:00,130 --> 00:48:01,440
is lambda one delta.

867
00:48:01,440 --> 00:48:08,210
But the green one doesn't is
one, minus lambda two delta.

868
00:48:08,210 --> 00:48:12,840
Here the probability would be
that the red one does not,

869
00:48:12,840 --> 00:48:16,400
times the probability that
the green one does.

870
00:48:16,400 --> 00:48:20,750
And then here we have the
probability that none of them

871
00:48:20,750 --> 00:48:26,790
flash, which is whatever
is left.

872
00:48:26,790 --> 00:48:29,600
But it's one minus lambda
one delta, times one

873
00:48:29,600 --> 00:48:33,160
minus lambda two delta.

874
00:48:33,160 --> 00:48:36,920
Now we're thinking about
delta as small.

875
00:48:36,920 --> 00:48:43,260
So think of the case where delta
goes to zero, but in a

876
00:48:43,260 --> 00:48:49,840
way that we keep the
first order terms.

877
00:48:49,840 --> 00:48:54,070
We keep the delta terms, but
we throw away the delta

878
00:48:54,070 --> 00:48:55,020
squared terms.

879
00:48:55,020 --> 00:48:58,170
Delta squared terms are much
smaller than the delta terms

880
00:48:58,170 --> 00:49:00,260
when delta becomes small.

881
00:49:00,260 --> 00:49:01,920
If we do that--

882
00:49:01,920 --> 00:49:05,650
if we only keep the order
of delta terms--

883
00:49:05,650 --> 00:49:07,940
this term effectively
disappears.

884
00:49:07,940 --> 00:49:09,110
This is delta squared.

885
00:49:09,110 --> 00:49:11,550
So we make it zero.

886
00:49:11,550 --> 00:49:14,550
So the probability of having
simultaneously a red and a

887
00:49:14,550 --> 00:49:17,940
green flash during a little
interval is negligible.

888
00:49:17,940 --> 00:49:20,150
What do we get here?

889
00:49:20,150 --> 00:49:23,200
Lambda delta times
one survives, but

890
00:49:23,200 --> 00:49:24,910
this times that doesn't.

891
00:49:24,910 --> 00:49:28,820
So we can throw that away.

892
00:49:28,820 --> 00:49:32,190
So the approximation that we
get is lambda one delta.

893
00:49:32,190 --> 00:49:34,010
Similarly here, this
goes away.

894
00:49:34,010 --> 00:49:36,420
We're left with a lambda
two delta.

895
00:49:36,420 --> 00:49:42,140
And this is whatever remains,
whatever is left.

896
00:49:42,140 --> 00:49:45,000
So what do we have?

897
00:49:45,000 --> 00:49:51,400
That there is a probability of
seeing a flash, either a red

898
00:49:51,400 --> 00:49:54,360
or a green, which is
lambda one delta,

899
00:49:54,360 --> 00:49:57,020
plus lambda two delta.

900
00:49:57,020 --> 00:50:03,590
So if we take a little interval
of length delta here,

901
00:50:03,590 --> 00:50:11,780
it's going to see an arrival
with probability approximately

902
00:50:11,780 --> 00:50:15,100
lambda one, plus lambda
two, delta.

903
00:50:15,100 --> 00:50:20,940
So every slot in this merged
process has an arrival

904
00:50:20,940 --> 00:50:25,780
probability with a rate which
is the sum of the rates of

905
00:50:25,780 --> 00:50:27,600
these two processes.

906
00:50:27,600 --> 00:50:29,640
So this is one part
of the definition

907
00:50:29,640 --> 00:50:31,680
of the Poisson process.

908
00:50:31,680 --> 00:50:34,890
There's a few more things that
one would need to verify.

909
00:50:34,890 --> 00:50:37,980
Namely, that intervals of the
same length have the same

910
00:50:37,980 --> 00:50:41,000
probability distribution and
that different slots are

911
00:50:41,000 --> 00:50:42,710
independent of each other.

912
00:50:42,710 --> 00:50:50,780
This can be argued by starting
from here because different

913
00:50:50,780 --> 00:50:53,620
intervals in this process are
independent from each other.

914
00:50:53,620 --> 00:50:56,900
Different intervals here are
independent from each other.

915
00:50:56,900 --> 00:50:59,900
It's not hard to argue that
different intervals in the

916
00:50:59,900 --> 00:51:03,580
merged process will also be
independent of each other.

917
00:51:03,580 --> 00:51:06,480
So the conclusion that comes
at the end is that this

918
00:51:06,480 --> 00:51:10,130
process is a Poisson process,
with a total rate which is

919
00:51:10,130 --> 00:51:13,210
equal to the sum of the rate
of the two processes.

920
00:51:13,210 --> 00:51:17,010
And now if I tell you that an
arrival happened in the merged

921
00:51:17,010 --> 00:51:20,530
process at a certain time,
how likely is it that

922
00:51:20,530 --> 00:51:23,470
it came from here?

923
00:51:23,470 --> 00:51:24,950
How likely is it?

924
00:51:24,950 --> 00:51:26,980
We go to this picture.

925
00:51:26,980 --> 00:51:30,140
Given that an arrival
occurred--

926
00:51:30,140 --> 00:51:36,050
which is the event that this
or that happened--

927
00:51:36,050 --> 00:51:39,330
what is the probability that
it came from the first

928
00:51:39,330 --> 00:51:42,060
process, the red one?

929
00:51:42,060 --> 00:51:45,190
Well it's the probability
of this divided by the

930
00:51:45,190 --> 00:51:48,030
probability of this,
times that.

931
00:51:48,030 --> 00:51:52,760
Given that this event occurred,
you want to find the

932
00:51:52,760 --> 00:51:56,560
conditional probability
of that sub event.

933
00:51:56,560 --> 00:51:58,960
So we're asking the question,
out of the total probability

934
00:51:58,960 --> 00:52:00,660
of these two, what
fraction of that

935
00:52:00,660 --> 00:52:02,790
probability is assigned here?

936
00:52:02,790 --> 00:52:05,300
And this is lambda one
delta, after we

937
00:52:05,300 --> 00:52:07,040
ignore the other terms.

938
00:52:07,040 --> 00:52:09,170
This is lambda two delta.

939
00:52:09,170 --> 00:52:15,040
So that fraction is going to be
lambda one, over lambda one

940
00:52:15,040 --> 00:52:16,770
plus lambda two.

941
00:52:16,770 --> 00:52:17,640
What does this tell you?

942
00:52:17,640 --> 00:52:21,820
If lambda one and lambda two are
equal, given that I saw an

943
00:52:21,820 --> 00:52:25,580
arrival here, it's equally
likely to be red or green.

944
00:52:25,580 --> 00:52:29,716
But if the reds have a much
higher arrival rate, when I

945
00:52:29,716 --> 00:52:32,700
see an arrival here, it's
more likely this

946
00:52:32,700 --> 00:52:34,050
number will be large.

947
00:52:34,050 --> 00:52:38,390
So it's more likely to have
come from the red process.

948
00:52:38,390 --> 00:52:40,830
OK so we'll continue with
this story and do some

949
00:52:40,830 --> 00:52:42,080
applications next time.