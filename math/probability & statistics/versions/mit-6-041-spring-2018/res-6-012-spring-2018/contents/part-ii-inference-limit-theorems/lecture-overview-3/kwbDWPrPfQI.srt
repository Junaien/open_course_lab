1
00:00:01,360 --> 00:00:05,400
If our objective is to keep the
mean squared estimation

2
00:00:05,400 --> 00:00:09,490
error small, then the best
possible estimator is the

3
00:00:09,490 --> 00:00:11,590
conditional expectation.

4
00:00:11,590 --> 00:00:12,920
But sometimes the conditional

5
00:00:12,920 --> 00:00:15,820
expectation is hard to calculate.

6
00:00:15,820 --> 00:00:18,410
Maybe we're missing the
details of the various

7
00:00:18,410 --> 00:00:20,340
probability distributions.

8
00:00:20,340 --> 00:00:24,080
Or maybe we have the
distributions that we need but

9
00:00:24,080 --> 00:00:26,690
the formulas are complicated.

10
00:00:26,690 --> 00:00:29,070
After all, the conditional
expectation can be a

11
00:00:29,070 --> 00:00:33,220
complicated non-linear function
of the observations.

12
00:00:33,220 --> 00:00:36,970
For this reason, we may want to
consider an estimator that

13
00:00:36,970 --> 00:00:40,910
has a simpler structure, an
estimator that is a linear

14
00:00:40,910 --> 00:00:42,700
function of the data.

15
00:00:42,700 --> 00:00:46,700
And then, within this class of
estimators, find the one that

16
00:00:46,700 --> 00:00:51,410
results in the smallest possible
mean squared error.

17
00:00:51,410 --> 00:00:54,870
In this lecture we will
formulate this linear least

18
00:00:54,870 --> 00:00:58,560
squares estimation problem
and then solve it.

19
00:00:58,560 --> 00:01:01,940
We will see that the solution
is given by a simple formula

20
00:01:01,940 --> 00:01:06,910
that involves only the means,
variances, and covariances of

21
00:01:06,910 --> 00:01:09,390
the random variables involved.

22
00:01:09,390 --> 00:01:13,060
Because of the simplicity of the
method, linear estimators

23
00:01:13,060 --> 00:01:16,340
are used quite often, especially
in systems where

24
00:01:16,340 --> 00:01:20,289
estimates need to be computed
quickly in real time as

25
00:01:20,289 --> 00:01:23,580
observations are obtained.

26
00:01:23,580 --> 00:01:26,710
We will look into some of the
mathematical properties of the

27
00:01:26,710 --> 00:01:30,650
linear least mean squares
estimator and the associated

28
00:01:30,650 --> 00:01:34,200
mean squared error, revisit an
example from the previous

29
00:01:34,200 --> 00:01:37,820
lecture, and finally close with
some comments on the ways

30
00:01:37,820 --> 00:01:39,759
that this estimator
can be used.