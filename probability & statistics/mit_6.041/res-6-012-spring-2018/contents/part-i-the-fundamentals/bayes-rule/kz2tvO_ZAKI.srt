1
00:00:02,310 --> 00:00:06,930
We now come to the third and
final kind of calculation out

2
00:00:06,930 --> 00:00:08,780
of the calculations that
we carried out

3
00:00:08,780 --> 00:00:10,640
in our earlier example.

4
00:00:10,640 --> 00:00:13,110
The setting is exactly the same
as in our discussion of

5
00:00:13,110 --> 00:00:15,000
the total probability theorem.

6
00:00:15,000 --> 00:00:19,040
We have a sample space which is
partitioned into a number

7
00:00:19,040 --> 00:00:22,090
of disjoint subsets
or events which

8
00:00:22,090 --> 00:00:24,330
we think of as scenarios.

9
00:00:24,330 --> 00:00:27,440
We're given the probability
of each scenario.

10
00:00:27,440 --> 00:00:30,790
And we think of these
probabilities as being some

11
00:00:30,790 --> 00:00:32,800
kind of initial beliefs.

12
00:00:32,800 --> 00:00:41,140
They capture how likely we
believe each scenario to be.

13
00:00:41,140 --> 00:00:47,050
Now, under each scenario, we
also have the probability that

14
00:00:47,050 --> 00:00:51,520
an event of interest,
event B, will occur.

15
00:00:51,520 --> 00:00:54,920
Then the probabilistic
experiment is carried out.

16
00:00:54,920 --> 00:01:00,100
And perhaps we observe that
event B did indeed occur.

17
00:01:00,100 --> 00:01:05,239
Once that happens, maybe this
should cause us to revise our

18
00:01:05,239 --> 00:01:09,050
beliefs about the likelihood
of the different scenarios.

19
00:01:09,050 --> 00:01:13,039
Having observed that B occurred,
perhaps certain

20
00:01:13,039 --> 00:01:16,820
scenarios are more likely
than others.

21
00:01:16,820 --> 00:01:18,510
How do we revise our beliefs?

22
00:01:18,510 --> 00:01:21,220
By calculating conditional
probabilities.

23
00:01:21,220 --> 00:01:24,340
And how do we calculate
conditional probabilities?

24
00:01:24,340 --> 00:01:28,130
We start from the definition of
conditional probabilities.

25
00:01:28,130 --> 00:01:31,520
The probability of one event
given another is the

26
00:01:31,520 --> 00:01:35,700
probability that both events
occur divided by the

27
00:01:35,700 --> 00:01:39,180
probability of the conditioning
event.

28
00:01:39,180 --> 00:01:41,450
How do we continue?

29
00:01:41,450 --> 00:01:45,200
We simply realize that the
numerator is what we can

30
00:01:45,200 --> 00:01:48,229
calculate using the
multiplication rule.

31
00:01:48,229 --> 00:01:52,590
And the denominator is exactly
what we calculate using the

32
00:01:52,590 --> 00:01:54,720
total probability theorem.

33
00:01:54,720 --> 00:01:58,860
So we have everything we need
to calculate those revised

34
00:01:58,860 --> 00:02:01,740
beliefs, or conditional
probabilities.

35
00:02:01,740 --> 00:02:04,340
And this all there is
in the Bayes rule.

36
00:02:04,340 --> 00:02:06,730
It is actually a very
simple calculation.

37
00:02:09,720 --> 00:02:11,710
It's a very simple
calculation.

38
00:02:11,710 --> 00:02:14,900
However, it is a quite
important one.

39
00:02:14,900 --> 00:02:17,650
Its history goes way back.

40
00:02:17,650 --> 00:02:20,960
In the middle of the 18th
century, a Presbyterian

41
00:02:20,960 --> 00:02:24,310
minister, Thomas Bayes,
worked it out.

42
00:02:24,310 --> 00:02:27,350
It was published a few years
after his death.

43
00:02:27,350 --> 00:02:30,650
And it was quickly reorganized
for its significance.

44
00:02:30,650 --> 00:02:34,370
It's a systematic way for
incorporating new evidence.

45
00:02:34,370 --> 00:02:38,220
It's a systematic way for
learning from experience.

46
00:02:38,220 --> 00:02:42,090
And it forms the foundation of a
major branch of mathematics,

47
00:02:42,090 --> 00:02:46,090
so-called Bayesian inference,
which we will study in some

48
00:02:46,090 --> 00:02:49,880
detail later in this course.

49
00:02:49,880 --> 00:02:53,860
The general idea is that we
start with a probabilistic

50
00:02:53,860 --> 00:02:57,160
model, which involves a number
of possible scenarios.

51
00:02:57,160 --> 00:03:00,820
And we have some initial beliefs
on the likelihood of

52
00:03:00,820 --> 00:03:04,430
each possible scenario.

53
00:03:04,430 --> 00:03:08,590
There's also some particular
event that may occur under

54
00:03:08,590 --> 00:03:09,600
each scenario.

55
00:03:09,600 --> 00:03:13,450
And we know how likely it is to
occur under each scenario.

56
00:03:13,450 --> 00:03:15,800
This is our model of
the situation.

57
00:03:15,800 --> 00:03:19,120
Under each particular situation,
the model tells us

58
00:03:19,120 --> 00:03:22,272
how likely event
B is to occur.

59
00:03:22,272 --> 00:03:27,350
If we actually observe that B
occurred, then we use that

60
00:03:27,350 --> 00:03:31,180
information to draw conclusions
about the possible

61
00:03:31,180 --> 00:03:36,820
causes of B, or conclusions
about the more likely or less

62
00:03:36,820 --> 00:03:41,579
likely scenarios that may have
caused this events to occur.

63
00:03:41,579 --> 00:03:44,050
That's what inference is.

64
00:03:44,050 --> 00:03:49,450
Having observed b, we make
inferences as to how likely a

65
00:03:49,450 --> 00:03:52,670
particular scenario,
Ai, is going to be.

66
00:03:52,670 --> 00:03:55,490
And that likelihood is captured
by this conditional

67
00:03:55,490 --> 00:04:00,260
probabilities of Ai, given the
event B. So that's what the

68
00:04:00,260 --> 00:04:02,000
Bayes rule is doing.

69
00:04:02,000 --> 00:04:04,430
Starting from conditional
probabilities going in one

70
00:04:04,430 --> 00:04:07,020
direction, it allows us to
calculate conditional

71
00:04:07,020 --> 00:04:11,320
probabilities going in the
opposite direction.

72
00:04:11,320 --> 00:04:15,340
It allows us to revise the
probabilities of the different

73
00:04:15,340 --> 00:04:18,540
scenarios, taking into account
the new information.

74
00:04:18,540 --> 00:04:22,520
And that's exactly what
inference is all about, as

75
00:04:22,520 --> 00:04:25,430
we're going to see later
in this class.