1
00:00:00,610 --> 00:00:03,100
We have seen so far two ways of

2
00:00:03,100 --> 00:00:05,810
estimating an unknown parameter.

3
00:00:05,810 --> 00:00:08,470
We can use the maximum a
posteriori probability

4
00:00:08,470 --> 00:00:12,110
estimate, or we can use the
conditional expectation.

5
00:00:12,110 --> 00:00:16,350
That is, the mean of the
posterior distribution.

6
00:00:16,350 --> 00:00:19,900
These were, in some sense,
arbitrary choices.

7
00:00:19,900 --> 00:00:23,350
How about imposing a performance
criterion, and

8
00:00:23,350 --> 00:00:26,890
then finding an estimate which
is optimal with respect to

9
00:00:26,890 --> 00:00:28,720
that criterion?

10
00:00:28,720 --> 00:00:33,000
This is what we will be
doing in this lecture.

11
00:00:33,000 --> 00:00:36,310
We introduce a specific
performance criterion, the

12
00:00:36,310 --> 00:00:40,330
expected value of the squared
estimation error, and we look

13
00:00:40,330 --> 00:00:44,910
for an estimator that is optimal
under this criterion.

14
00:00:44,910 --> 00:00:48,510
It turns out that the optimal
estimator is the conditional

15
00:00:48,510 --> 00:00:49,850
expectation.

16
00:00:49,850 --> 00:00:53,370
And this is why we have been
calling it the least mean

17
00:00:53,370 --> 00:00:55,770
squares estimator.

18
00:00:55,770 --> 00:00:59,230
It plays a central role because
it is a canonical way

19
00:00:59,230 --> 00:01:02,230
of estimating unknown
random variables.

20
00:01:02,230 --> 00:01:06,010
We will study some of its
theoretical properties, and we

21
00:01:06,010 --> 00:01:09,490
will also illustrate its use and
the associated performance

22
00:01:09,490 --> 00:01:12,020
analysis in the context
of an example.